{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1: Principal Component Analysis (PCA)\n",
    "This section focuses on dimensionality reduction, moving from high-dimensional data to a 2D space for visualization and efficiency while maintaining the integrity of the information."
   ],
   "id": "92907e35960237fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Load or Create a Dataset\n",
    "We will use the Wine Dataset, which has 13 features (dimensions). This is more complex than a simple 2D or 3D plot and allows us to see the power of PCA."
   ],
   "id": "c39c10ee8ae8068d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T08:54:43.789606387Z",
     "start_time": "2026-02-04T08:54:42.030175185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# PCA is sensitive to scaling, so we must standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Original shape: {X.shape}\")"
   ],
   "id": "3320a69c3460e3af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (178, 13)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Find the First 2 Principal Components (Manual vs. Scikit-Learn)\n",
    "\n",
    "#### Manual Method (Eigendecomposition)\n",
    "\n",
    "To compute without scikit-learn:\n",
    "1. Calculate the Covariance Matrix\n",
    "2. Find the Eigenvectors (directions of maximum variance)\n",
    "3. Project the data onto the eigenvectors"
   ],
   "id": "3b2172041fbefe6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T09:24:41.627650397Z",
     "start_time": "2026-02-04T09:24:41.170745823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A) WITHOUT SKLEARN (Manual)\n",
    "# 1. Compute covariance matrix\n",
    "cov_matrix = np.cov(X_scaled.T)\n",
    "\n",
    "# 2. Get eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "print(\"eigenvectors:\", eigenvectors)\n",
    "print(\"eigenvalues:\", eigenvalues)\n",
    "\n",
    "\n",
    "# 3. Sort eigenvectors by eigenvalues in descending order\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "print(\"sorted eigenvectors:\", eigenvectors)\n",
    "\n",
    "# 4. Project data onto the first 2 eigenvectors\n",
    "X_pca_manual = X_scaled.dot(eigenvectors[:, :2])\n",
    "\n",
    "# B) WITH SKLEARN\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "\n",
    "X_pca_sklearn = pca.fit_transform(X_scaled)\n",
    "print(\"Sklearn PCA:\", X_pca_sklearn)\n",
    "\n",
    "print(\"Manual PCA shape:\", X_pca_manual.shape)\n",
    "print(\"Sklearn PCA shape:\", X_pca_sklearn.shape)"
   ],
   "id": "245acff5bb192791",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvectors: [[-0.1443294   0.48365155  0.20738262 -0.0178563  -0.26566365  0.21353865\n",
      "   0.05639636 -0.01496997  0.39613926 -0.26628645 -0.50861912 -0.22591696\n",
      "   0.21160473]\n",
      " [ 0.24518758  0.22493093 -0.08901289  0.53689028  0.03521363  0.53681385\n",
      "  -0.42052391 -0.02596375  0.06582674  0.12169604  0.07528304  0.07648554\n",
      "  -0.30907994]\n",
      " [ 0.00205106  0.31606881 -0.6262239  -0.21417556 -0.14302547  0.15447466\n",
      "   0.14917061  0.14121803 -0.17026002 -0.04962237  0.30769445 -0.49869142\n",
      "  -0.02712539]\n",
      " [ 0.23932041 -0.0105905  -0.61208035  0.06085941  0.06610294 -0.10082451\n",
      "   0.28696914 -0.09168285  0.42797018 -0.05574287 -0.20044931  0.47931378\n",
      "   0.05279942]\n",
      " [-0.14199204  0.299634   -0.13075693 -0.35179658  0.72704851  0.03814394\n",
      "  -0.3228833  -0.05677422 -0.15636143  0.06222011 -0.27140257  0.07128891\n",
      "   0.06787022]\n",
      " [-0.39466085  0.06503951 -0.14617896  0.19806835 -0.14931841 -0.0841223\n",
      "   0.02792498  0.46390791 -0.40593409 -0.30388245 -0.28603452  0.30434119\n",
      "  -0.32013135]\n",
      " [-0.4229343  -0.00335981 -0.1506819   0.15229479 -0.10902584 -0.01892002\n",
      "   0.06068521 -0.83225706 -0.18724536 -0.04289883 -0.04957849 -0.02569409\n",
      "  -0.16315051]\n",
      " [ 0.2985331   0.02877949 -0.17036816 -0.20330102 -0.50070298 -0.25859401\n",
      "  -0.59544729 -0.11403985 -0.23328465  0.04235219 -0.19550132  0.11689586\n",
      "   0.21553507]\n",
      " [-0.31342949  0.03930172 -0.14945431  0.39905653  0.13685982 -0.53379539\n",
      "  -0.37213935  0.11691707  0.36822675 -0.09555303  0.20914487 -0.23736257\n",
      "   0.1341839 ]\n",
      " [ 0.0886167   0.52999567  0.13730621  0.06592568 -0.07643678 -0.41864414\n",
      "   0.22771214  0.0119928  -0.03379692  0.60422163 -0.05621752  0.0318388\n",
      "  -0.29077518]\n",
      " [-0.29671456 -0.27923515 -0.08522192 -0.42777141 -0.17361452  0.10598274\n",
      "  -0.23207564  0.08988884  0.43662362  0.259214   -0.08582839 -0.04821201\n",
      "  -0.52239889]\n",
      " [-0.37616741 -0.16449619 -0.16600459  0.18412074 -0.10116099  0.26585107\n",
      "   0.0447637   0.15671813 -0.07810789  0.60095872 -0.1372269   0.0464233\n",
      "   0.52370587]\n",
      " [-0.28675223  0.36490283  0.12674592 -0.23207086 -0.1578688   0.11972557\n",
      "  -0.0768045  -0.01444734  0.12002267 -0.07940162  0.57578611  0.53926983\n",
      "   0.162116  ]]\n",
      "eigenvalues: [4.73243698 2.51108093 1.45424187 0.92416587 0.85804868 0.64528221\n",
      " 0.55414147 0.10396199 0.35046627 0.16972374 0.29051203 0.22706428\n",
      " 0.25232001]\n",
      "sorted eigenvectors: [[-0.1443294   0.48365155  0.20738262 -0.0178563  -0.26566365  0.21353865\n",
      "   0.05639636  0.39613926 -0.50861912  0.21160473 -0.22591696 -0.26628645\n",
      "  -0.01496997]\n",
      " [ 0.24518758  0.22493093 -0.08901289  0.53689028  0.03521363  0.53681385\n",
      "  -0.42052391  0.06582674  0.07528304 -0.30907994  0.07648554  0.12169604\n",
      "  -0.02596375]\n",
      " [ 0.00205106  0.31606881 -0.6262239  -0.21417556 -0.14302547  0.15447466\n",
      "   0.14917061 -0.17026002  0.30769445 -0.02712539 -0.49869142 -0.04962237\n",
      "   0.14121803]\n",
      " [ 0.23932041 -0.0105905  -0.61208035  0.06085941  0.06610294 -0.10082451\n",
      "   0.28696914  0.42797018 -0.20044931  0.05279942  0.47931378 -0.05574287\n",
      "  -0.09168285]\n",
      " [-0.14199204  0.299634   -0.13075693 -0.35179658  0.72704851  0.03814394\n",
      "  -0.3228833  -0.15636143 -0.27140257  0.06787022  0.07128891  0.06222011\n",
      "  -0.05677422]\n",
      " [-0.39466085  0.06503951 -0.14617896  0.19806835 -0.14931841 -0.0841223\n",
      "   0.02792498 -0.40593409 -0.28603452 -0.32013135  0.30434119 -0.30388245\n",
      "   0.46390791]\n",
      " [-0.4229343  -0.00335981 -0.1506819   0.15229479 -0.10902584 -0.01892002\n",
      "   0.06068521 -0.18724536 -0.04957849 -0.16315051 -0.02569409 -0.04289883\n",
      "  -0.83225706]\n",
      " [ 0.2985331   0.02877949 -0.17036816 -0.20330102 -0.50070298 -0.25859401\n",
      "  -0.59544729 -0.23328465 -0.19550132  0.21553507  0.11689586  0.04235219\n",
      "  -0.11403985]\n",
      " [-0.31342949  0.03930172 -0.14945431  0.39905653  0.13685982 -0.53379539\n",
      "  -0.37213935  0.36822675  0.20914487  0.1341839  -0.23736257 -0.09555303\n",
      "   0.11691707]\n",
      " [ 0.0886167   0.52999567  0.13730621  0.06592568 -0.07643678 -0.41864414\n",
      "   0.22771214 -0.03379692 -0.05621752 -0.29077518  0.0318388   0.60422163\n",
      "   0.0119928 ]\n",
      " [-0.29671456 -0.27923515 -0.08522192 -0.42777141 -0.17361452  0.10598274\n",
      "  -0.23207564  0.43662362 -0.08582839 -0.52239889 -0.04821201  0.259214\n",
      "   0.08988884]\n",
      " [-0.37616741 -0.16449619 -0.16600459  0.18412074 -0.10116099  0.26585107\n",
      "   0.0447637  -0.07810789 -0.1372269   0.52370587  0.0464233   0.60095872\n",
      "   0.15671813]\n",
      " [-0.28675223  0.36490283  0.12674592 -0.23207086 -0.1578688   0.11972557\n",
      "  -0.0768045   0.12002267  0.57578611  0.162116    0.53926983 -0.07940162\n",
      "  -0.01444734]]\n",
      "Sklearn PCA: [[ 3.31675081  1.44346263]\n",
      " [ 2.20946492 -0.33339289]\n",
      " [ 2.51674015  1.0311513 ]\n",
      " [ 3.75706561  2.75637191]\n",
      " [ 1.00890849  0.86983082]\n",
      " [ 3.05025392  2.12240111]\n",
      " [ 2.44908967  1.17485013]\n",
      " [ 2.05943687  1.60896307]\n",
      " [ 2.5108743   0.91807096]\n",
      " [ 2.75362819  0.78943767]\n",
      " [ 3.47973668  1.30233324]\n",
      " [ 1.7547529   0.61197723]\n",
      " [ 2.11346234  0.67570634]\n",
      " [ 3.45815682  1.13062988]\n",
      " [ 4.31278391  2.09597558]\n",
      " [ 2.3051882   1.66255173]\n",
      " [ 2.17195527  2.32730534]\n",
      " [ 1.89897118  1.63136888]\n",
      " [ 3.54198508  2.51834367]\n",
      " [ 2.0845222   1.06113799]\n",
      " [ 3.12440254  0.78689711]\n",
      " [ 1.08657007  0.24174355]\n",
      " [ 2.53522408 -0.09184062]\n",
      " [ 1.64498834 -0.51627893]\n",
      " [ 1.76157587 -0.31714893]\n",
      " [ 0.9900791   0.94066734]\n",
      " [ 1.77527763  0.68617513]\n",
      " [ 1.23542396 -0.08980704]\n",
      " [ 2.18840633  0.68956962]\n",
      " [ 2.25610898  0.19146194]\n",
      " [ 2.50022003  1.24083383]\n",
      " [ 2.67741105  1.47187365]\n",
      " [ 1.62857912  0.05270445]\n",
      " [ 1.90269086  1.63306043]\n",
      " [ 1.41038853  0.69793432]\n",
      " [ 1.90382623  0.17671095]\n",
      " [ 1.38486223  0.65863985]\n",
      " [ 1.12220741  0.11410976]\n",
      " [ 1.5021945  -0.76943201]\n",
      " [ 2.52980109  1.80300198]\n",
      " [ 2.58809543  0.7796163 ]\n",
      " [ 0.66848199  0.16996094]\n",
      " [ 3.07080699  1.15591896]\n",
      " [ 0.46220914  0.33074213]\n",
      " [ 2.10135193 -0.07100892]\n",
      " [ 1.13616618  1.77710739]\n",
      " [ 2.72660096  1.19133469]\n",
      " [ 2.82133927  0.6462586 ]\n",
      " [ 2.00985085  1.24702946]\n",
      " [ 2.7074913   1.75196741]\n",
      " [ 3.21491747  0.16699199]\n",
      " [ 2.85895983  0.7452788 ]\n",
      " [ 3.50560436  1.61273386]\n",
      " [ 2.22479138  1.875168  ]\n",
      " [ 2.14698782  1.01675154]\n",
      " [ 2.46932948  1.32900831]\n",
      " [ 2.74151791  1.43654878]\n",
      " [ 2.17374092  1.21219984]\n",
      " [ 3.13938015  1.73157912]\n",
      " [-0.92858197 -3.07348616]\n",
      " [-1.54248014 -1.38144351]\n",
      " [-1.83624976 -0.82998412]\n",
      " [ 0.03060683 -1.26278614]\n",
      " [ 2.05026161 -1.9250326 ]\n",
      " [-0.60968083 -1.90805881]\n",
      " [ 0.90022784 -0.76391147]\n",
      " [ 2.24850719 -1.88459248]\n",
      " [ 0.18338403 -2.42714611]\n",
      " [-0.81280503 -0.22051399]\n",
      " [ 1.9756205  -1.40328323]\n",
      " [-1.57221622 -0.88498314]\n",
      " [ 1.65768181 -0.9567122 ]\n",
      " [-0.72537239 -1.0636454 ]\n",
      " [ 2.56222717  0.26019855]\n",
      " [ 1.83256757 -1.2878782 ]\n",
      " [-0.8679929  -2.44410119]\n",
      " [ 0.3700144  -2.15390698]\n",
      " [-1.45737704 -1.38335177]\n",
      " [ 1.26293085 -0.77084953]\n",
      " [ 0.37615037 -1.0270434 ]\n",
      " [ 0.7620639  -3.37505381]\n",
      " [ 1.03457797 -1.45070974]\n",
      " [-0.49487676 -2.38124353]\n",
      " [-2.53897708 -0.08744336]\n",
      " [ 0.83532015 -1.47367055]\n",
      " [ 0.78790461 -2.02662652]\n",
      " [-0.80683216 -2.23383039]\n",
      " [-0.55804262 -2.37298543]\n",
      " [-1.11511104 -1.80224719]\n",
      " [-0.55572283 -2.65754004]\n",
      " [-1.34928528 -2.11800147]\n",
      " [-1.56448261 -1.85221452]\n",
      " [-1.93255561 -1.55949546]\n",
      " [ 0.74666594 -2.31293171]\n",
      " [ 0.95745536 -2.22352843]\n",
      " [ 2.54386518  0.16927402]\n",
      " [-0.54395259 -0.36892655]\n",
      " [ 1.03104975 -2.56556935]\n",
      " [ 2.25190942 -1.43274138]\n",
      " [ 1.41021602 -2.16619177]\n",
      " [ 0.79771979 -2.3769488 ]\n",
      " [-0.54953173 -2.29312864]\n",
      " [-0.16117374 -1.16448332]\n",
      " [-0.65979494 -2.67996119]\n",
      " [ 0.39235441 -2.09873171]\n",
      " [-1.77249908 -1.71728847]\n",
      " [-0.36626736 -2.1693533 ]\n",
      " [-1.62067257 -1.35558339]\n",
      " [ 0.08253578 -2.30623459]\n",
      " [ 1.57827507 -1.46203429]\n",
      " [ 1.42056925 -1.41820664]\n",
      " [-0.27870275 -1.93056809]\n",
      " [-1.30314497 -0.76317231]\n",
      " [-0.45707187 -2.26941561]\n",
      " [-0.49418585 -1.93904505]\n",
      " [ 0.48207441 -3.87178385]\n",
      " [-0.25288888 -2.82149237]\n",
      " [-0.10722764 -1.92892204]\n",
      " [-2.4330126  -1.25714104]\n",
      " [-0.55108954 -2.22216155]\n",
      " [ 0.73962193 -1.40895667]\n",
      " [ 1.33632173  0.25333693]\n",
      " [-1.177087   -0.66396684]\n",
      " [-0.46233501 -0.61828818]\n",
      " [ 0.97847408 -1.4455705 ]\n",
      " [-0.09680973 -2.10999799]\n",
      " [ 0.03848715 -1.26676211]\n",
      " [-1.5971585  -1.20814357]\n",
      " [-0.47956492 -1.93884066]\n",
      " [-1.79283347 -1.1502881 ]\n",
      " [-1.32710166  0.17038923]\n",
      " [-2.38450083  0.37458261]\n",
      " [-2.9369401   0.26386183]\n",
      " [-2.14681113  0.36825495]\n",
      " [-2.36986949 -0.45963481]\n",
      " [-3.06384157  0.35341284]\n",
      " [-3.91575378  0.15458252]\n",
      " [-3.93646339  0.65968723]\n",
      " [-3.09427612  0.34884276]\n",
      " [-2.37447163  0.29198035]\n",
      " [-2.77881295  0.28680487]\n",
      " [-2.28656128  0.37250784]\n",
      " [-2.98563349  0.48921791]\n",
      " [-2.3751947   0.48233372]\n",
      " [-2.20986553  1.1600525 ]\n",
      " [-2.625621    0.56316076]\n",
      " [-4.28063878  0.64967096]\n",
      " [-3.58264137  1.27270275]\n",
      " [-2.80706372  1.57053379]\n",
      " [-2.89965933  2.04105701]\n",
      " [-2.32073698  2.35636608]\n",
      " [-2.54983095  2.04528309]\n",
      " [-1.81254128  1.52764595]\n",
      " [-2.76014464  2.13893235]\n",
      " [-2.7371505   0.40988627]\n",
      " [-3.60486887  1.80238422]\n",
      " [-2.889826    1.92521861]\n",
      " [-3.39215608  1.31187639]\n",
      " [-1.0481819   3.51508969]\n",
      " [-1.60991228  2.40663816]\n",
      " [-3.14313097  0.73816104]\n",
      " [-2.2401569   1.17546529]\n",
      " [-2.84767378  0.55604397]\n",
      " [-2.59749706  0.69796554]\n",
      " [-2.94929937  1.55530896]\n",
      " [-3.53003227  0.8825268 ]\n",
      " [-2.40611054  2.59235618]\n",
      " [-2.92908473  1.27444695]\n",
      " [-2.18141278  2.07753731]\n",
      " [-2.38092779  2.58866743]\n",
      " [-3.21161722 -0.2512491 ]\n",
      " [-3.67791872  0.84774784]\n",
      " [-2.4655558   2.1937983 ]\n",
      " [-3.37052415  2.21628914]\n",
      " [-2.60195585  1.75722935]\n",
      " [-2.67783946  2.76089913]\n",
      " [-2.38701709  2.29734668]\n",
      " [-3.20875816  2.76891957]]\n",
      "Manual PCA shape: (178, 2)\n",
      "Sklearn PCA shape: (178, 2)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Preserving a Certain Percentage of Variance\n",
    "\n",
    "Instead of selecting a fixed number of components (e.g., 2), PCA can be configured to retain enough components to explain a specified percentage of the data's variance, such as 95%."
   ],
   "id": "2b884508bb2066d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pca_95 = PCA(n_components=0.95)\n",
    "X_reduced_95 = pca_95.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Components needed for 95% variance: {pca_95.n_components_}\")"
   ],
   "id": "1ebe40715647520f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Train a Neural Network: Original vs. PCA\n",
    "\n",
    "We use a Multi-Layer Perceptron (MLP) to compare model performance on the original dataset versus the PCA-transformed dataset."
   ],
   "id": "20c1c175b84d73ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# a) Original Dataset\n",
    "mlp_orig = MLPClassifier(max_iter=500, random_state=42)\n",
    "mlp_orig.fit(X_train, y_train)\n",
    "acc_orig = accuracy_score(y_test, mlp_orig.predict(X_test))\n",
    "\n",
    "# b) PCA Components (using the 2 components found earlier)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca_sklearn, y, test_size=0.2, random_state=42)\n",
    "mlp_pca = MLPClassifier(max_iter=500, random_state=42)\n",
    "mlp_pca.fit(X_train_pca, y_train_pca)\n",
    "acc_pca = accuracy_score(y_test_pca, mlp_pca.predict(X_test_pca))\n",
    "\n",
    "print(f\"Accuracy (Original - 13 features): {acc_orig:.4f}\")\n",
    "print(f\"Accuracy (PCA - 2 features): {acc_pca:.4f}\")"
   ],
   "id": "2ee59f33d2292e41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Kernel PCA (Linear, Sigmoid, RBF)\n",
    "\n",
    "Standard PCA captures only linear relationships. Kernel PCA applies the kernel trick to identify non-linear patterns, using kernels such as Linear, Sigmoid, and RBF."
   ],
   "id": "580bd558550bcb19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# List of kernels to iterate through\n",
    "# 'linear' is equivalent to standard PCA\n",
    "# 'sigmoid' is inspired by neural networks\n",
    "# 'rbf' (Radial Basis Function) is great for circular/complex clusters\n",
    "kernels = [\"linear\", \"sigmoid\", \"rbf\"]\n",
    "\n",
    "for k in kernels:\n",
    "    # 1. Initialize KernelPCA with 2 components and the current kernel\n",
    "    kpca = KernelPCA(n_components=2, kernel=k)\n",
    "    X_kpca = kpca.fit_transform(X_scaled)\n",
    "\n",
    "    # 2. Split the newly transformed data into training and testing sets\n",
    "    X_train_k, X_test_k, y_train_k, y_test_k = train_test_split(\n",
    "        X_kpca, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # 3. Initialize and train the MLP Classifier on the KPCA-transformed data\n",
    "    # We increase max_iter to ensure the network converges\n",
    "    mlp_k = MLPClassifier(max_iter=1000, random_state=42).fit(X_train_k, y_train_k)\n",
    "\n",
    "    # 4. Predict and print the results\n",
    "    predictions = mlp_k.predict(X_test_k)\n",
    "    accuracy = accuracy_score(y_test_k, predictions)\n",
    "    print(f\"Kernel: {k:8} | Accuracy: {accuracy:.4f}\")"
   ],
   "id": "c3e57d022bc83e9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Hyperparameter Tuning Pipeline\n",
    "\n",
    "We implement a Pipeline combined with GridSearchCV to simultaneously optimize the kernel selection and neural network hyperparameters."
   ],
   "id": "cf6f8379723f56b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. Define the Pipeline\n",
    "# This sequences the steps: First apply KPCA, then feed the result into the MLP\n",
    "pipeline = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"mlp\", MLPClassifier(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# 2. Define the Parameter Grid\n",
    "# Note the naming convention: 'stepname__parametername'\n",
    "param_grid = {\n",
    "    # Tuning KPCA: testing different kernels and the gamma (influence) parameter\n",
    "    \"kpca__kernel\": [\"linear\", \"rbf\", \"sigmoid\"],\n",
    "    \"kpca__gamma\": [0.01, 0.1, 1],\n",
    "\n",
    "    # Tuning the Neural Network: testing different architectures and activations\n",
    "    \"mlp__hidden_layer_sizes\": [(50,), (100,), (50, 50)], # Try 1 or 2 hidden layers\n",
    "    \"mlp__activation\": [\"relu\", \"tanh\"] # Relu is standard, Tanh is often good for scaled data\n",
    "}\n",
    "\n",
    "# 3. Initialize GridSearchCV\n",
    "# cv=3 means 3-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1) # n_jobs=-1 uses all CPU cores\n",
    "\n",
    "# 4. Run the search on the scaled data\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# 5. Output the best combination found\n",
    "print(\"Best Parameters Found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# 6. Output the best score achieved during training\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")"
   ],
   "id": "a7f6834a700042ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: Classification with Keras\n",
    "\n",
    "**IRIS vs. Fashion MNIST**: IRIS is a \"toy\" dataset (150 samples, 4 features) where deep learning is often overkill. Fashion MNIST (70,000 images) is the real test for your Neural Network.\n",
    "\n",
    "**One-Hot Encoding**: For IRIS, because we used `categorical_crossentropy`, we had to transform the label `2` into `[0, 0, 1]`. For Fashion MNIST, we used `sparse_categorical_crossentropy`, which allows us to keep labels as integers.\n",
    "\n",
    "**Callbacks**: These are your safety net. `EarlyStopping` prevents your model from \"memorizing\" (overfitting) the training data by halting training when the test loss starts to increase."
   ],
   "id": "5d67436f0e9d7de1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Load IRIS and Fashion MNIST\n",
    "While Fashion MNIST is a standard Keras dataset, the IRIS dataset is usually loaded via sklearn or pandas and then converted for Keras."
   ],
   "id": "33c14df8c9423eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# A) Load IRIS\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target.reshape(-1, 1)\n",
    "\n",
    "# Preprocess IRIS: Scale features and One-Hot encode labels for the NN\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_iris_cat = encoder.fit_transform(y_iris)\n",
    "\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris_scaled, y_iris_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# B) Load Fashion MNIST\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_f, y_train_f), (X_test_f, y_test_f) = fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess Fashion MNIST: Scale to [0, 1] and Flatten (for a simple Dense NN)\n",
    "X_train_f = X_train_f / 255.0\n",
    "X_test_f = X_test_f / 255.0"
   ],
   "id": "7a896dfcac602b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Build and Train the Neural Network\n",
    "We'll create a function to build the model so we can reuse it for tuning."
   ],
   "id": "295a7de96139885c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_simple_model(input_shape, num_classes, hidden_size=64, activation='relu'):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=input_shape),\n",
    "        keras.layers.Dense(hidden_size, activation=activation),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy' if num_classes > 2 else 'sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training on Fashion MNIST (as it's more complex)\n",
    "# Note: Using sparse_categorical_crossentropy because labels are integers (0-9)\n",
    "model_f = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_f.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model_f.fit(X_train_f, y_train_f, epochs=10,\n",
    "                    validation_data=(X_test_f, y_test_f), verbose=1)"
   ],
   "id": "6cad53356bf4f645"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Hyperparameter Tuning (What else can be tuned?)\n",
    "\n",
    "Beyond `hidden_layer_sizes` and `activation`, you can tune:\n",
    "\n",
    "- **Optimizer**: Adam, SGD, RMSprop\n",
    "- **Learning Rate**: One of the most critical parameters\n",
    "- **Batch Size**: 32, 64, 128 (impacts stability and speed)\n",
    "- **Dropout Rate**: To prevent overfitting\n",
    "- **Regularization (L1/L2)**: Penalizes large weights"
   ],
   "id": "dd9a058c3951f37f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Plot Loss and Accuracy\n",
    "Visualizing the history object is key to spotting overfitting."
   ],
   "id": "2b419f7324ed82a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_history(history):\n",
    "    # Create a figure with two subplots\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ],
   "id": "e794a973cdac1920"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Callbacks and Saving Weights\n",
    "Callbacks allow the model to \"do things\" during training, like saving only the best version of itself."
   ],
   "id": "5babf32ed47f4bf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define callbacks\n",
    "# ModelCheckpoint: Saves weights during training\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_best_weights.weights.h5\",\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True)\n",
    "\n",
    "# EarlyStopping: Stops training if the model stops improving\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Re-train with callbacks\n",
    "model_f.fit(X_train_f, y_train_f, epochs=20,\n",
    "            validation_data=(X_test_f, y_test_f),\n",
    "            callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "# Manually save weights\n",
    "model_f.save_weights(\"final_weights.weights.h5\")"
   ],
   "id": "12341e33cea11368"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Saving and Loading the Full Model\n",
    "This saves the architecture, weights, AND optimizer state."
   ],
   "id": "67a6153477e2f173"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the entire model\n",
    "model_f.save(\"my_fashion_model.keras\")\n",
    "\n",
    "# Load it back\n",
    "loaded_model = keras.models.load_model(\"my_fashion_model.keras\")\n",
    "\n",
    "# Verify by evaluating\n",
    "loss, acc = loaded_model.evaluate(X_test_f, y_test_f, verbose=0)\n",
    "print(f\"Loaded model accuracy: {acc:.4f}\")"
   ],
   "id": "86fb3beb88c60a1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task 3: Regression with California Housing",
   "id": "5d22a03c9b040d86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Load and Split the Dataset\n",
    "The California housing dataset contains features like median income and house age to predict the median house value."
   ],
   "id": "9588c062e3fef3f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# 2. Split into training and testing (80/20)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create a validation set from the training set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale the features (essential for Neural Networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training shape: {X_train.shape}\")"
   ],
   "id": "fc7c523db1ccf0fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Build and Train the Neural Network\n",
    "For regression, we use Mean Squared Error (MSE) as the loss function. The output layer has 1 neuron with no activation function to allow it to predict any continuous value."
   ],
   "id": "6fd595c173155e19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1) # Output layer for regression (no activation)\n",
    "])\n",
    "\n",
    "# Compile with Mean Squared Error (MSE)\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ],
   "id": "d118dbc19691faa8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Plot the Network History",
   "id": "526b74797673e0fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regression_history(history, title=\"Training History\"):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1) # MSE usually drops quickly\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "plot_regression_history(history)"
   ],
   "id": "6ca37ce05cb9a360"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Change Learning Rate and Compare\n",
    "Changing the learning rate is one of the most impactful tuning steps. A rate too high causes the model to diverge; a rate too low makes it take forever to learn."
   ],
   "id": "6356688f2e489e94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a new model instance to reset weights\n",
    "model_lr = keras.models.clone_model(model)\n",
    "\n",
    "# Compile with a much higher learning rate (e.g., 0.1)\n",
    "model_lr.compile(loss=\"mean_squared_error\",\n",
    "                 optimizer=keras.optimizers.Adam(learning_rate=0.1))\n",
    "\n",
    "# Train again\n",
    "history_lr = model_lr.fit(X_train, y_train, epochs=20,\n",
    "                          validation_data=(X_valid, y_valid), verbose=0)\n",
    "\n",
    "plot_regression_history(history_lr, title=\"Training History (Learning Rate = 0.1)\")\n",
    "\n",
    "# Discussion: Usually, 0.1 is too high for Adam on this dataset,\n",
    "# and you'll see the loss \"jump\" around or fail to decrease."
   ],
   "id": "72a4241c3d9801f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Callbacks and Saving Weights",
   "id": "a4bd045ce69c221c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define Checkpoint to save best weights\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"housing_weights.weights.h5\",\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True)\n",
    "\n",
    "# Early Stopping stops training when the validation loss hasn't improved for 5 epochs\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train with callbacks\n",
    "history_final = model.fit(X_train, y_train, epochs=50,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          callbacks=[checkpoint_cb, early_stopping_cb])"
   ],
   "id": "19f1818f038644f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Save and Load the Model",
   "id": "3bca7bd35d2b550f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the full model (Architecture + Weights + Optimizer state)\n",
    "model.save(\"california_housing_model.keras\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"california_housing_model.keras\")\n",
    "\n",
    "# Test prediction\n",
    "sample_data = X_test[:3]\n",
    "predictions = loaded_model.predict(sample_data)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual values:\", y_test[:3])"
   ],
   "id": "6a98d187ee151cf5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
