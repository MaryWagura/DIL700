{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1: Principal Component Analysis (PCA)\n",
    "This section focuses on dimensionality reduction, moving from high-dimensional data to a 2D space for visualization and efficiency while maintaining the integrity of the information."
   ],
   "id": "92907e35960237fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Load or Create a Dataset\n",
    "We will use the Wine Dataset, which has 13 features (dimensions). This is more complex than a simple 2D or 3D plot and allows us to see the power of PCA."
   ],
   "id": "c39c10ee8ae8068d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T08:54:43.789606387Z",
     "start_time": "2026-02-04T08:54:42.030175185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# PCA is sensitive to scaling, so we must standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Original shape: {X.shape}\")"
   ],
   "id": "3320a69c3460e3af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (178, 13)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Find the First 2 Principal Components (Manual vs. Scikit-Learn)\n",
    "\n",
    "#### Manual Method (Eigendecomposition)\n",
    "\n",
    "To compute without scikit-learn:\n",
    "1. Calculate the Covariance Matrix\n",
    "2. Find the Eigenvectors (directions of maximum variance)\n",
    "3. Project the data onto the eigenvectors"
   ],
   "id": "3b2172041fbefe6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# A) WITHOUT SKLEARN (Manual)\n",
    "# 1. Compute covariance matrix\n",
    "cov_matrix = np.cov(X_scaled.T)\n",
    "\n",
    "# 2. Get eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# 3. Sort eigenvectors by eigenvalues in descending order\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "# 4. Project data onto the first 2 eigenvectors\n",
    "X_pca_manual = X_scaled.dot(eigenvectors[:, :2])\n",
    "\n",
    "# B) WITH SKLEARN\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca_sklearn = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Manual PCA shape:\", X_pca_manual.shape)\n",
    "print(\"Sklearn PCA shape:\", X_pca_sklearn.shape)"
   ],
   "id": "245acff5bb192791"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Preserving a Certain Percentage of Variance\n",
    "\n",
    "Instead of selecting a fixed number of components (e.g., 2), PCA can be configured to retain enough components to explain a specified percentage of the data's variance, such as 95%."
   ],
   "id": "2b884508bb2066d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pca_95 = PCA(n_components=0.95)\n",
    "X_reduced_95 = pca_95.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Components needed for 95% variance: {pca_95.n_components_}\")"
   ],
   "id": "1ebe40715647520f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Train a Neural Network: Original vs. PCA\n",
    "\n",
    "We use a Multi-Layer Perceptron (MLP) to compare model performance on the original dataset versus the PCA-transformed dataset."
   ],
   "id": "20c1c175b84d73ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# a) Original Dataset\n",
    "mlp_orig = MLPClassifier(max_iter=500, random_state=42)\n",
    "mlp_orig.fit(X_train, y_train)\n",
    "acc_orig = accuracy_score(y_test, mlp_orig.predict(X_test))\n",
    "\n",
    "# b) PCA Components (using the 2 components found earlier)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca_sklearn, y, test_size=0.2, random_state=42)\n",
    "mlp_pca = MLPClassifier(max_iter=500, random_state=42)\n",
    "mlp_pca.fit(X_train_pca, y_train_pca)\n",
    "acc_pca = accuracy_score(y_test_pca, mlp_pca.predict(X_test_pca))\n",
    "\n",
    "print(f\"Accuracy (Original - 13 features): {acc_orig:.4f}\")\n",
    "print(f\"Accuracy (PCA - 2 features): {acc_pca:.4f}\")"
   ],
   "id": "2ee59f33d2292e41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Kernel PCA (Linear, Sigmoid, RBF)\n",
    "\n",
    "Standard PCA captures only linear relationships. Kernel PCA applies the kernel trick to identify non-linear patterns, using kernels such as Linear, Sigmoid, and RBF."
   ],
   "id": "580bd558550bcb19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# List of kernels to iterate through\n",
    "# 'linear' is equivalent to standard PCA\n",
    "# 'sigmoid' is inspired by neural networks\n",
    "# 'rbf' (Radial Basis Function) is great for circular/complex clusters\n",
    "kernels = [\"linear\", \"sigmoid\", \"rbf\"]\n",
    "\n",
    "for k in kernels:\n",
    "    # 1. Initialize KernelPCA with 2 components and the current kernel\n",
    "    kpca = KernelPCA(n_components=2, kernel=k)\n",
    "    X_kpca = kpca.fit_transform(X_scaled)\n",
    "\n",
    "    # 2. Split the newly transformed data into training and testing sets\n",
    "    X_train_k, X_test_k, y_train_k, y_test_k = train_test_split(\n",
    "        X_kpca, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # 3. Initialize and train the MLP Classifier on the KPCA-transformed data\n",
    "    # We increase max_iter to ensure the network converges\n",
    "    mlp_k = MLPClassifier(max_iter=1000, random_state=42).fit(X_train_k, y_train_k)\n",
    "\n",
    "    # 4. Predict and print the results\n",
    "    predictions = mlp_k.predict(X_test_k)\n",
    "    accuracy = accuracy_score(y_test_k, predictions)\n",
    "    print(f\"Kernel: {k:8} | Accuracy: {accuracy:.4f}\")"
   ],
   "id": "c3e57d022bc83e9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Hyperparameter Tuning Pipeline\n",
    "\n",
    "We implement a Pipeline combined with GridSearchCV to simultaneously optimize the kernel selection and neural network hyperparameters."
   ],
   "id": "cf6f8379723f56b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. Define the Pipeline\n",
    "# This sequences the steps: First apply KPCA, then feed the result into the MLP\n",
    "pipeline = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"mlp\", MLPClassifier(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# 2. Define the Parameter Grid\n",
    "# Note the naming convention: 'stepname__parametername'\n",
    "param_grid = {\n",
    "    # Tuning KPCA: testing different kernels and the gamma (influence) parameter\n",
    "    \"kpca__kernel\": [\"linear\", \"rbf\", \"sigmoid\"],\n",
    "    \"kpca__gamma\": [0.01, 0.1, 1],\n",
    "\n",
    "    # Tuning the Neural Network: testing different architectures and activations\n",
    "    \"mlp__hidden_layer_sizes\": [(50,), (100,), (50, 50)], # Try 1 or 2 hidden layers\n",
    "    \"mlp__activation\": [\"relu\", \"tanh\"] # Relu is standard, Tanh is often good for scaled data\n",
    "}\n",
    "\n",
    "# 3. Initialize GridSearchCV\n",
    "# cv=3 means 3-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1) # n_jobs=-1 uses all CPU cores\n",
    "\n",
    "# 4. Run the search on the scaled data\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# 5. Output the best combination found\n",
    "print(\"Best Parameters Found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# 6. Output the best score achieved during training\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")"
   ],
   "id": "a7f6834a700042ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: Classification with Keras\n",
    "\n",
    "**IRIS vs. Fashion MNIST**: IRIS is a \"toy\" dataset (150 samples, 4 features) where deep learning is often overkill. Fashion MNIST (70,000 images) is the real test for your Neural Network.\n",
    "\n",
    "**One-Hot Encoding**: For IRIS, because we used `categorical_crossentropy`, we had to transform the label `2` into `[0, 0, 1]`. For Fashion MNIST, we used `sparse_categorical_crossentropy`, which allows us to keep labels as integers.\n",
    "\n",
    "**Callbacks**: These are your safety net. `EarlyStopping` prevents your model from \"memorizing\" (overfitting) the training data by halting training when the test loss starts to increase."
   ],
   "id": "5d67436f0e9d7de1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Load IRIS and Fashion MNIST\n",
    "While Fashion MNIST is a standard Keras dataset, the IRIS dataset is usually loaded via sklearn or pandas and then converted for Keras."
   ],
   "id": "33c14df8c9423eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# A) Load IRIS\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target.reshape(-1, 1)\n",
    "\n",
    "# Preprocess IRIS: Scale features and One-Hot encode labels for the NN\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_iris_cat = encoder.fit_transform(y_iris)\n",
    "\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris_scaled, y_iris_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# B) Load Fashion MNIST\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_f, y_train_f), (X_test_f, y_test_f) = fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess Fashion MNIST: Scale to [0, 1] and Flatten (for a simple Dense NN)\n",
    "X_train_f = X_train_f / 255.0\n",
    "X_test_f = X_test_f / 255.0"
   ],
   "id": "7a896dfcac602b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Build and Train the Neural Network\n",
    "We'll create a function to build the model so we can reuse it for tuning."
   ],
   "id": "295a7de96139885c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_simple_model(input_shape, num_classes, hidden_size=64, activation='relu'):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=input_shape),\n",
    "        keras.layers.Dense(hidden_size, activation=activation),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy' if num_classes > 2 else 'sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training on Fashion MNIST (as it's more complex)\n",
    "# Note: Using sparse_categorical_crossentropy because labels are integers (0-9)\n",
    "model_f = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_f.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model_f.fit(X_train_f, y_train_f, epochs=10,\n",
    "                    validation_data=(X_test_f, y_test_f), verbose=1)"
   ],
   "id": "6cad53356bf4f645"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Hyperparameter Tuning (What else can be tuned?)\n",
    "\n",
    "Beyond `hidden_layer_sizes` and `activation`, you can tune:\n",
    "\n",
    "- **Optimizer**: Adam, SGD, RMSprop\n",
    "- **Learning Rate**: One of the most critical parameters\n",
    "- **Batch Size**: 32, 64, 128 (impacts stability and speed)\n",
    "- **Dropout Rate**: To prevent overfitting\n",
    "- **Regularization (L1/L2)**: Penalizes large weights"
   ],
   "id": "dd9a058c3951f37f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Plot Loss and Accuracy\n",
    "Visualizing the history object is key to spotting overfitting."
   ],
   "id": "2b419f7324ed82a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_history(history):\n",
    "    # Create a figure with two subplots\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ],
   "id": "e794a973cdac1920"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Callbacks and Saving Weights\n",
    "Callbacks allow the model to \"do things\" during training, like saving only the best version of itself."
   ],
   "id": "5babf32ed47f4bf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define callbacks\n",
    "# ModelCheckpoint: Saves weights during training\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_best_weights.weights.h5\",\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True)\n",
    "\n",
    "# EarlyStopping: Stops training if the model stops improving\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Re-train with callbacks\n",
    "model_f.fit(X_train_f, y_train_f, epochs=20,\n",
    "            validation_data=(X_test_f, y_test_f),\n",
    "            callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "# Manually save weights\n",
    "model_f.save_weights(\"final_weights.weights.h5\")"
   ],
   "id": "12341e33cea11368"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Saving and Loading the Full Model\n",
    "This saves the architecture, weights, AND optimizer state."
   ],
   "id": "67a6153477e2f173"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the entire model\n",
    "model_f.save(\"my_fashion_model.keras\")\n",
    "\n",
    "# Load it back\n",
    "loaded_model = keras.models.load_model(\"my_fashion_model.keras\")\n",
    "\n",
    "# Verify by evaluating\n",
    "loss, acc = loaded_model.evaluate(X_test_f, y_test_f, verbose=0)\n",
    "print(f\"Loaded model accuracy: {acc:.4f}\")"
   ],
   "id": "86fb3beb88c60a1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task 3: Regression with California Housing",
   "id": "5d22a03c9b040d86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Load and Split the Dataset\n",
    "The California housing dataset contains features like median income and house age to predict the median house value."
   ],
   "id": "9588c062e3fef3f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# 2. Split into training and testing (80/20)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create a validation set from the training set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Scale the features (essential for Neural Networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training shape: {X_train.shape}\")"
   ],
   "id": "fc7c523db1ccf0fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Build and Train the Neural Network\n",
    "For regression, we use Mean Squared Error (MSE) as the loss function. The output layer has 1 neuron with no activation function to allow it to predict any continuous value."
   ],
   "id": "6fd595c173155e19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(1) # Output layer for regression (no activation)\n",
    "])\n",
    "\n",
    "# Compile with Mean Squared Error (MSE)\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ],
   "id": "d118dbc19691faa8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Plot the Network History",
   "id": "526b74797673e0fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regression_history(history, title=\"Training History\"):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1) # MSE usually drops quickly\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "plot_regression_history(history)"
   ],
   "id": "6ca37ce05cb9a360"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Change Learning Rate and Compare\n",
    "Changing the learning rate is one of the most impactful tuning steps. A rate too high causes the model to diverge; a rate too low makes it take forever to learn."
   ],
   "id": "6356688f2e489e94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a new model instance to reset weights\n",
    "model_lr = keras.models.clone_model(model)\n",
    "\n",
    "# Compile with a much higher learning rate (e.g., 0.1)\n",
    "model_lr.compile(loss=\"mean_squared_error\",\n",
    "                 optimizer=keras.optimizers.Adam(learning_rate=0.1))\n",
    "\n",
    "# Train again\n",
    "history_lr = model_lr.fit(X_train, y_train, epochs=20,\n",
    "                          validation_data=(X_valid, y_valid), verbose=0)\n",
    "\n",
    "plot_regression_history(history_lr, title=\"Training History (Learning Rate = 0.1)\")\n",
    "\n",
    "# Discussion: Usually, 0.1 is too high for Adam on this dataset,\n",
    "# and you'll see the loss \"jump\" around or fail to decrease."
   ],
   "id": "72a4241c3d9801f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Callbacks and Saving Weights",
   "id": "a4bd045ce69c221c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define Checkpoint to save best weights\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"housing_weights.weights.h5\",\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True)\n",
    "\n",
    "# Early Stopping stops training when the validation loss hasn't improved for 5 epochs\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train with callbacks\n",
    "history_final = model.fit(X_train, y_train, epochs=50,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          callbacks=[checkpoint_cb, early_stopping_cb])"
   ],
   "id": "19f1818f038644f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Save and Load the Model",
   "id": "3bca7bd35d2b550f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the full model (Architecture + Weights + Optimizer state)\n",
    "model.save(\"california_housing_model.keras\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"california_housing_model.keras\")\n",
    "\n",
    "# Test prediction\n",
    "sample_data = X_test[:3]\n",
    "predictions = loaded_model.predict(sample_data)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual values:\", y_test[:3])"
   ],
   "id": "6a98d187ee151cf5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
