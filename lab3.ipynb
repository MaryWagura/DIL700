{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: MNIST with Optimal Learning Rate & TensorBoard\n",
    "\n",
    "## Step 1: MNIST Optimization\n",
    "\n",
    "**Objective:** Reach >98% accuracy on the MNIST dataset.\n",
    "\n",
    "### Learning Rate Finder\n",
    "\n",
    "We implement a callback that exponentially increases the learning rate during training. By plotting Loss vs. Learning Rate, we can identify the optimal learning rate range—typically 10x smaller than the rate where the loss begins to increase.\n",
    "\n",
    "### TensorBoard Integration\n",
    "\n",
    "TensorBoard provides real-time visualization of training metrics including:\n",
    "- Loss curves\n",
    "- Accuracy metrics\n",
    "- Learning rate progression\n",
    "- Model graph structure"
   ],
   "id": "ae50e1734afb1775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Data Loading and Preprocessing",
   "id": "2a2e2fbd742bc558"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:18:09.087794658Z",
     "start_time": "2026-02-20T08:18:02.919236246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 1. Load MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# 2. Scale pixel values to [0, 1] range\n",
    "# Neural networks perform better with small, normalized input values\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# 3. Create a validation set (last 5,000 images)\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "print(f\"Data Loaded: {len(X_train)} training samples, {len(X_valid)} validation samples\")"
   ],
   "id": "d1b55da27f635ea4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 09:18:03.805316: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded: 55000 training samples, 5000 validation samples\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Define the Learning Rate Finder Callback",
   "id": "3e7269c32ca918cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:18:23.301104265Z",
     "start_time": "2026-02-20T08:18:23.158953151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom callback to grow learning rate exponentially during training\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__() # Ensure parent class is initialized\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # 1. Access the learning rate variable directly from the optimizer\n",
    "        lr_variable = self.model.optimizer.learning_rate\n",
    "\n",
    "        # 2. Convert to float safely (Works across Keras 2 and 3)\n",
    "        current_lr = float(lr_variable)\n",
    "\n",
    "        self.rates.append(current_lr)\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "\n",
    "        # 3. Use .assign() to update the learning rate for the next batch\n",
    "        # This is the modern, safe way to change variables in Keras/TensorFlow\n",
    "        lr_variable.assign(current_lr * self.factor)"
   ],
   "id": "f2caebc82464a331",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Find the Optimal Learning Rate",
   "id": "5ab51786282dce2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:18:39.463799083Z",
     "start_time": "2026-02-20T08:18:29.654164882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Build a basic MLP model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 2. Compile with a low starting LR\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Run the LR finder for 1 epoch\n",
    "expon_lr = ExponentialLearningRate(factor=1.005)\n",
    "model.fit(X_train, y_train, epochs=1, callbacks=[expon_lr])\n",
    "\n",
    "# 4. Plot the results\n",
    "plt.plot(expon_lr.rates, expon_lr.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.axis([1e-3, 10, 0, 5])\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Finding the Optimal Learning Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "51899719544425ba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clauds/anaconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 5ms/step - accuracy: 0.5878 - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Final Training with TensorBoard and Precision MetricNote: Look at your plot above. Usually, a good choice is about 1/10th of the rate where the loss started to explode. For MNIST, this is often around $3 \\times 10^{-1}$.",
   "id": "b8813446d08bb956"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "In deep learning laboratory settings, labels are typically transformed using **One-Hot Encoding**. This converts a single numeric label (e.g., class `5`) into a binary vector:\n"
   ],
   "id": "9d35614c38af85ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:21:29.141100972Z",
     "start_time": "2026-02-20T08:18:50.607455463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. One-Hot Encode the labels to fix the Precision metric shape error\n",
    "# This turns integers (0-9) into 10-element vectors\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_valid_cat = keras.utils.to_categorical(y_valid, num_classes=10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# 2. Define TensorBoard log directory\n",
    "root_logdir = os.path.join(os.curdir, \"my_mnist_logs\")\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(root_logdir)\n",
    "\n",
    "# 3. Re-build/Reset the model\n",
    "model = keras.models.clone_model(model)\n",
    "\n",
    "# 4. Compile with Categorical Crossentropy and standard Precision\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", # Changed from sparse because of one-hot labels\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=3e-1),\n",
    "    metrics=[\"accuracy\", keras.metrics.Precision(name=\"precision\")]\n",
    ")\n",
    "\n",
    "# 5. Final Training\n",
    "# We use the categorical labels here\n",
    "history = model.fit(X_train, y_train_cat, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid_cat),\n",
    "                    callbacks=[tensorboard_cb])\n",
    "\n",
    "# 6. Evaluate on Test Set\n",
    "test_results = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"\\nFinal Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"Final Test Precision: {test_results[2]:.4f}\")"
   ],
   "id": "f732adb825f26f5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - accuracy: 0.9282 - loss: 0.2315 - precision: 0.9489 - val_accuracy: 0.9706 - val_loss: 0.1016 - val_precision: 0.9777\n",
      "Epoch 2/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step - accuracy: 0.9711 - loss: 0.0950 - precision: 0.9753 - val_accuracy: 0.9746 - val_loss: 0.0870 - val_precision: 0.9794\n",
      "Epoch 3/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 4ms/step - accuracy: 0.9786 - loss: 0.0669 - precision: 0.9812 - val_accuracy: 0.9742 - val_loss: 0.0907 - val_precision: 0.9773\n",
      "Epoch 4/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step - accuracy: 0.9849 - loss: 0.0476 - precision: 0.9863 - val_accuracy: 0.9772 - val_loss: 0.0836 - val_precision: 0.9793\n",
      "Epoch 5/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 4ms/step - accuracy: 0.9884 - loss: 0.0360 - precision: 0.9895 - val_accuracy: 0.9778 - val_loss: 0.0739 - val_precision: 0.9797\n",
      "Epoch 6/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step - accuracy: 0.9908 - loss: 0.0289 - precision: 0.9916 - val_accuracy: 0.9816 - val_loss: 0.0688 - val_precision: 0.9834\n",
      "Epoch 7/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 0.9928 - loss: 0.0225 - precision: 0.9933 - val_accuracy: 0.9838 - val_loss: 0.0702 - val_precision: 0.9842\n",
      "Epoch 8/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 6ms/step - accuracy: 0.9945 - loss: 0.0173 - precision: 0.9948 - val_accuracy: 0.9824 - val_loss: 0.0821 - val_precision: 0.9832\n",
      "Epoch 9/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step - accuracy: 0.9957 - loss: 0.0129 - precision: 0.9958 - val_accuracy: 0.9850 - val_loss: 0.0774 - val_precision: 0.9854\n",
      "Epoch 10/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step - accuracy: 0.9947 - loss: 0.0164 - precision: 0.9949 - val_accuracy: 0.9816 - val_loss: 0.0797 - val_precision: 0.9830\n",
      "Epoch 11/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step - accuracy: 0.9960 - loss: 0.0119 - precision: 0.9962 - val_accuracy: 0.9760 - val_loss: 0.1088 - val_precision: 0.9766\n",
      "Epoch 12/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 0.9957 - loss: 0.0123 - precision: 0.9959 - val_accuracy: 0.9846 - val_loss: 0.0748 - val_precision: 0.9848\n",
      "Epoch 13/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 0.9967 - loss: 0.0115 - precision: 0.9969 - val_accuracy: 0.9846 - val_loss: 0.0855 - val_precision: 0.9858\n",
      "Epoch 14/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 0.9978 - loss: 0.0071 - precision: 0.9979 - val_accuracy: 0.9844 - val_loss: 0.0822 - val_precision: 0.9852\n",
      "Epoch 15/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 4ms/step - accuracy: 0.9978 - loss: 0.0070 - precision: 0.9978 - val_accuracy: 0.9828 - val_loss: 0.0796 - val_precision: 0.9842\n",
      "Epoch 16/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 0.9996 - loss: 0.0018 - precision: 0.9996 - val_accuracy: 0.9868 - val_loss: 0.0764 - val_precision: 0.9868\n",
      "Epoch 17/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 1.0000 - loss: 3.8052e-04 - precision: 1.0000 - val_accuracy: 0.9872 - val_loss: 0.0758 - val_precision: 0.9874\n",
      "Epoch 18/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step - accuracy: 1.0000 - loss: 1.1038e-04 - precision: 1.0000 - val_accuracy: 0.9882 - val_loss: 0.0768 - val_precision: 0.9886\n",
      "Epoch 19/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 1.0000 - loss: 8.1306e-05 - precision: 1.0000 - val_accuracy: 0.9884 - val_loss: 0.0772 - val_precision: 0.9888\n",
      "Epoch 20/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 1.0000 - loss: 6.8524e-05 - precision: 1.0000 - val_accuracy: 0.9888 - val_loss: 0.0779 - val_precision: 0.9892\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.9864 - loss: 0.0768 - precision: 0.9869\n",
      "\n",
      "Final Test Accuracy: 0.9864\n",
      "Final Test Precision: 0.9869\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 2: The 100-Layer Challenge & Vanishing Gradients\n",
    "\n",
    "## Step 2: Deep Architecture Analysis\n",
    "\n",
    "**Objective:** Understanding why modern architectures need specialized activation functions.\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "In very deep networks, gradients diminish as they propagate backward through layers. By the time the gradient signal reaches the initial layers, it approaches zero—effectively stopping the model from learning in early layers.\n",
    "\n",
    "#### 1. The \"Vanishing Gradient\" Trap (Sigmoid)\n",
    "If you train the 100-layer model with Sigmoid, you will likely see the accuracy stay stuck at exactly 10%.\n",
    "\n",
    "Why: In a 100-layer network, the \"signal\" from the error at the end of the model gets smaller and smaller as it travels backward toward the first layer.\n",
    "\n",
    "The Result: By the time the math reaches the early layers, the update value is so tiny (vanishing) that the weights never change. The model stays in a state of \"random guessing,\" and since there are 10 classes in MNIST, a random guess is 10% accurate.\n",
    "\n",
    "#### 2. The \"Self-Normalizing\" Miracle (SELU)\n",
    "If you switch to SELU (Scaled Exponential Linear Unit), the model should actually start learning.\n",
    "\n",
    "Why: SELU, when paired with kernel_initializer=\"lecun_normal\", has a special mathematical property: it keeps the mean and variance of the neuron outputs consistent across all 100 layers.\n",
    "\n",
    "The Result: The gradient doesn't explode or vanish, allowing the information to flow through all 100 layers. You will see the accuracy climb from 10% to 90%+.\n",
    "\n",
    "### Activation Function Comparison\n",
    "\n",
    "| Activation | Characteristics | Vanishing Gradient Issue |\n",
    "|------------|-----------------|--------------------------|\n",
    "| **Sigmoid** | Saturates at 0 or 1 | Severe - gradients vanish due to saturation |\n",
    "| **ReLU** | Positive values pass, negatives become 0 | Moderate - can cause \"Dying ReLU\" where neurons get stuck at 0 |\n",
    "| **ELU/SELU** | Allows negative values, keeps mean activation near zero | Minimal - SELU provides \"Self-Normalization\" for deep networks |"
   ],
   "id": "301e913566aada8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T08:27:18.022813599Z",
     "start_time": "2026-02-20T08:27:13.528001750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to build a super deep 100-layer model\n",
    "def build_deep_model(activation):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "    # Add 100 hidden layers\n",
    "    for _ in range(100):\n",
    "        model.add(keras.layers.Dense(100, activation=activation, kernel_initializer=\"he_normal\" if activation != \"sigmoid\" else \"glorot_uniform\"))\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "# Practice: Run this for 'sigmoid' then 'selu'\n",
    "# Note: Sigmoid will likely show 10% accuracy (random guessing) because it can't train 100 layers.\n",
    "# model_deep = build_deep_model(\"selu\")\n",
    "model_deep = build_deep_model(\"sigmoid\")\n",
    "model_deep.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "# model_deep.fit(...)"
   ],
   "id": "816e7df923644758",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 3: CIFAR10, Batch Normalization, and Optimizers\n",
    "\n",
    "## Step 3: CIFAR10 and Optimization\n",
    "\n",
    "**Objective:** Train on the more complex CIFAR10 dataset (color images, 3 channels) and address training stability issues.\n",
    "\n",
    "### He Initialization\n",
    "\n",
    "Designed specifically for ELU/ReLU activation functions to prevent signal death and maintain proper gradient flow throughout the network.\n",
    "\n",
    "### Batch Normalization (BN)\n",
    "\n",
    "Standardizes the inputs to each layer, providing:\n",
    "- Ability to use much higher learning rates\n",
    "- Reduced sensitivity to weight initialization\n",
    "- Faster convergence and improved training stability\n",
    "\n",
    "### Optimizer Comparison\n",
    "\n",
    "| Optimizer | Description | Best Use Case |\n",
    "|-----------|-------------|---------------|\n",
    "| **Momentum** | Builds velocity like a ball rolling downhill | Standard SGD with faster convergence |\n",
    "| **Adam/Nadam** | Combines momentum with adaptive learning rates per weight | The \"go-to\" optimizer for most deep learning tasks |"
   ],
   "id": "6deb5dd52e5d1474"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T16:28:20.980396260Z",
     "start_time": "2026-02-14T16:10:50.699230475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load CIFAR10\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "\n",
    "# 2. Build DNN with Batch Normalization\n",
    "model_cifar = keras.Sequential()\n",
    "model_cifar.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "# Add 20 layers with Batch Normalization\n",
    "for _ in range(20):\n",
    "    model_cifar.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\")) # Layer\n",
    "    model_cifar.add(keras.layers.BatchNormalization())                      # Normalization\n",
    "    model_cifar.add(keras.layers.Activation(\"elu\"))                        # Activation\n",
    "\n",
    "model_cifar.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# 3. Train with Nadam\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model_cifar.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Early Stopping to save time\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history_cifar = model_cifar.fit(X_train_full, y_train_full, epochs=50,\n",
    "                                validation_split=0.1, callbacks=[early_stopping_cb])"
   ],
   "id": "da2e38724e1a139b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 17:10:58.480078: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 552960000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 29ms/step - accuracy: 0.3379 - loss: 1.8422 - val_accuracy: 0.3714 - val_loss: 1.7833\n",
      "Epoch 2/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 19ms/step - accuracy: 0.4056 - loss: 1.6603 - val_accuracy: 0.3618 - val_loss: 1.7728\n",
      "Epoch 3/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 37ms/step - accuracy: 0.4335 - loss: 1.5919 - val_accuracy: 0.4034 - val_loss: 1.7095\n",
      "Epoch 4/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m57s\u001B[0m 40ms/step - accuracy: 0.4541 - loss: 1.5405 - val_accuracy: 0.4368 - val_loss: 1.5845\n",
      "Epoch 5/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 37ms/step - accuracy: 0.4690 - loss: 1.5067 - val_accuracy: 0.4168 - val_loss: 1.6746\n",
      "Epoch 6/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m63s\u001B[0m 23ms/step - accuracy: 0.4807 - loss: 1.4671 - val_accuracy: 0.4252 - val_loss: 1.6027\n",
      "Epoch 7/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 21ms/step - accuracy: 0.4925 - loss: 1.4353 - val_accuracy: 0.4770 - val_loss: 1.4744\n",
      "Epoch 8/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 23ms/step - accuracy: 0.5015 - loss: 1.4047 - val_accuracy: 0.4530 - val_loss: 1.5420\n",
      "Epoch 9/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 22ms/step - accuracy: 0.5135 - loss: 1.3766 - val_accuracy: 0.4638 - val_loss: 1.5134\n",
      "Epoch 10/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 21ms/step - accuracy: 0.5172 - loss: 1.3578 - val_accuracy: 0.4442 - val_loss: 1.5947\n",
      "Epoch 11/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 23ms/step - accuracy: 0.5269 - loss: 1.3312 - val_accuracy: 0.4794 - val_loss: 1.4783\n",
      "Epoch 12/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 21ms/step - accuracy: 0.5328 - loss: 1.3090 - val_accuracy: 0.5076 - val_loss: 1.4219\n",
      "Epoch 13/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 20ms/step - accuracy: 0.5459 - loss: 1.2927 - val_accuracy: 0.5032 - val_loss: 1.4313\n",
      "Epoch 14/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 19ms/step - accuracy: 0.5495 - loss: 1.2728 - val_accuracy: 0.4512 - val_loss: 1.6056\n",
      "Epoch 15/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 21ms/step - accuracy: 0.5563 - loss: 1.2553 - val_accuracy: 0.4972 - val_loss: 1.4334\n",
      "Epoch 16/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.5622 - loss: 1.2379 - val_accuracy: 0.4690 - val_loss: 1.5176\n",
      "Epoch 17/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.5675 - loss: 1.2199 - val_accuracy: 0.4810 - val_loss: 1.4996\n",
      "Epoch 18/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.5744 - loss: 1.2043 - val_accuracy: 0.4274 - val_loss: 1.7869\n",
      "Epoch 19/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.5780 - loss: 1.1899 - val_accuracy: 0.5026 - val_loss: 1.4426\n",
      "Epoch 20/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.5838 - loss: 1.1762 - val_accuracy: 0.5112 - val_loss: 1.4014\n",
      "Epoch 21/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 22ms/step - accuracy: 0.5902 - loss: 1.1581 - val_accuracy: 0.4780 - val_loss: 1.5385\n",
      "Epoch 22/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 17ms/step - accuracy: 0.5945 - loss: 1.1437 - val_accuracy: 0.4980 - val_loss: 1.4933\n",
      "Epoch 23/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 16ms/step - accuracy: 0.5961 - loss: 1.1394 - val_accuracy: 0.4876 - val_loss: 1.4838\n",
      "Epoch 24/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.6030 - loss: 1.1240 - val_accuracy: 0.4108 - val_loss: 1.9329\n",
      "Epoch 25/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 22ms/step - accuracy: 0.6098 - loss: 1.1081 - val_accuracy: 0.4834 - val_loss: 1.5184\n",
      "Epoch 26/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.6118 - loss: 1.0956 - val_accuracy: 0.5048 - val_loss: 1.4415\n",
      "Epoch 27/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 21ms/step - accuracy: 0.6180 - loss: 1.0841 - val_accuracy: 0.4910 - val_loss: 1.5153\n",
      "Epoch 28/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.6185 - loss: 1.0784 - val_accuracy: 0.4990 - val_loss: 1.5101\n",
      "Epoch 29/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 24ms/step - accuracy: 0.6254 - loss: 1.0669 - val_accuracy: 0.4934 - val_loss: 1.5433\n",
      "Epoch 30/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.6302 - loss: 1.0546 - val_accuracy: 0.4910 - val_loss: 1.5362\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparison Discussion\n",
    "\n",
    "### Convergence Speed\n",
    "Batch Normalization typically enables the model to reach higher accuracy in fewer epochs, even though each epoch may take slightly longer to compute due to the additional normalization calculations.\n",
    "\n",
    "### Optimizer Differences\n",
    "\n",
    "| Optimizer | Characteristics | Tuning Required |\n",
    "|-----------|-----------------|------------------|\n",
    "| **SGD** | Slow convergence, can get stuck in local minima | High |\n",
    "| **Momentum/NAG** | Faster than SGD, builds velocity, better at escaping local minima | Medium |\n",
    "| **Adam/Nadam** | Most \"forgiving\", combines momentum with adaptive learning rates, fastest convergence | Low |"
   ],
   "id": "2bf7ebc509364295"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
