{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: MNIST with Optimal Learning Rate & TensorBoard\n",
    "\n",
    "## Step 1: MNIST Optimization\n",
    "\n",
    "**Objective:** Reach >98% accuracy on the MNIST dataset.\n",
    "\n",
    "### Learning Rate Finder\n",
    "\n",
    "We implement a callback that exponentially increases the learning rate during training. By plotting Loss vs. Learning Rate, we can identify the optimal learning rate range—typically 10x smaller than the rate where the loss begins to increase.\n",
    "\n",
    "### TensorBoard Integration\n",
    "\n",
    "TensorBoard provides real-time visualization of training metrics including:\n",
    "- Loss curves\n",
    "- Accuracy metrics\n",
    "- Learning rate progression\n",
    "- Model graph structure"
   ],
   "id": "ae50e1734afb1775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Data Loading and Preprocessing",
   "id": "2a2e2fbd742bc558"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:33:18.441435135Z",
     "start_time": "2026-02-20T12:33:11.812745667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 1. Load MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# 2. Scale pixel values to [0, 1] range\n",
    "# Neural networks perform better with small, normalized input values\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# 3. Create a validation set (last 5,000 images)\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "print(f\"Data Loaded: {len(X_train)} training samples, {len(X_valid)} validation samples\")"
   ],
   "id": "d1b55da27f635ea4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 13:33:12.972902: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded: 55000 training samples, 5000 validation samples\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Define the Learning Rate Finder Callback",
   "id": "3e7269c32ca918cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:33:24.615872058Z",
     "start_time": "2026-02-20T12:33:24.543537171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom callback to grow learning rate exponentially during training\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__() # Ensure parent class is initialized\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # 1. Access the learning rate variable directly from the optimizer\n",
    "        lr_variable = self.model.optimizer.learning_rate\n",
    "\n",
    "        # 2. Convert to float safely (Works across Keras 2 and 3)\n",
    "        current_lr = float(lr_variable)\n",
    "\n",
    "        self.rates.append(current_lr)\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "\n",
    "        # 3. Use .assign() to update the learning rate for the next batch\n",
    "        # This is the modern, safe way to change variables in Keras/TensorFlow\n",
    "        lr_variable.assign(current_lr * self.factor)"
   ],
   "id": "f2caebc82464a331",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Find the Optimal Learning Rate",
   "id": "5ab51786282dce2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:33:37.761238755Z",
     "start_time": "2026-02-20T12:33:27.848281287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Build a basic MLP model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 2. Compile with a low starting LR\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Run the LR finder for 1 epoch\n",
    "expon_lr = ExponentialLearningRate(factor=1.005)\n",
    "model.fit(X_train, y_train, epochs=1, callbacks=[expon_lr])\n",
    "\n",
    "# 4. Plot the results\n",
    "plt.plot(expon_lr.rates, expon_lr.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.axis([1e-3, 10, 0, 5])\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Finding the Optimal Learning Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "51899719544425ba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clauds/anaconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 4ms/step - accuracy: 0.5956 - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Final Training with TensorBoard and Precision MetricNote: Look at your plot above. Usually, a good choice is about 1/10th of the rate where the loss started to explode. For MNIST, this is often around $3 \\times 10^{-1}$.",
   "id": "b8813446d08bb956"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "In deep learning laboratory settings, labels are typically transformed using **One-Hot Encoding**. This converts a single numeric label (e.g., class `5`) into a binary vector:\n"
   ],
   "id": "9d35614c38af85ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T12:36:33.047219514Z",
     "start_time": "2026-02-20T12:33:41.457188851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. One-Hot Encode the labels to fix the Precision metric shape error\n",
    "# This turns integers (0-9) into 10-element vectors\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_valid_cat = keras.utils.to_categorical(y_valid, num_classes=10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# 2. Define TensorBoard log directory\n",
    "root_logdir = os.path.join(os.curdir, \"my_mnist_logs\")\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(root_logdir)\n",
    "\n",
    "# 3. Re-build/Reset the model\n",
    "model = keras.models.clone_model(model)\n",
    "\n",
    "# 4. Compile with Categorical Crossentropy and standard Precision\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", # Changed from sparse because of one-hot labels\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=3e-1),\n",
    "    metrics=[\"accuracy\", keras.metrics.Precision(name=\"precision\")]\n",
    ")\n",
    "\n",
    "# 5. Final Training\n",
    "# We use the categorical labels here\n",
    "history = model.fit(X_train, y_train_cat, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid_cat),\n",
    "                    callbacks=[tensorboard_cb])\n",
    "\n",
    "# 6. Evaluate on Test Set\n",
    "test_results = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"\\nFinal Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"Final Test Precision: {test_results[2]:.4f}\")"
   ],
   "id": "f732adb825f26f5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - accuracy: 0.9279 - loss: 0.2338 - precision: 0.9484 - val_accuracy: 0.9720 - val_loss: 0.0942 - val_precision: 0.9776\n",
      "Epoch 2/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - accuracy: 0.9717 - loss: 0.0926 - precision: 0.9760 - val_accuracy: 0.9734 - val_loss: 0.0882 - val_precision: 0.9784\n",
      "Epoch 3/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 5ms/step - accuracy: 0.9794 - loss: 0.0641 - precision: 0.9824 - val_accuracy: 0.9772 - val_loss: 0.0780 - val_precision: 0.9807\n",
      "Epoch 4/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - accuracy: 0.9840 - loss: 0.0481 - precision: 0.9856 - val_accuracy: 0.9762 - val_loss: 0.0861 - val_precision: 0.9781\n",
      "Epoch 5/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - accuracy: 0.9888 - loss: 0.0350 - precision: 0.9897 - val_accuracy: 0.9822 - val_loss: 0.0724 - val_precision: 0.9837\n",
      "Epoch 6/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step - accuracy: 0.9911 - loss: 0.0282 - precision: 0.9920 - val_accuracy: 0.9836 - val_loss: 0.0643 - val_precision: 0.9848\n",
      "Epoch 7/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step - accuracy: 0.9929 - loss: 0.0216 - precision: 0.9934 - val_accuracy: 0.9814 - val_loss: 0.0743 - val_precision: 0.9820\n",
      "Epoch 8/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 4ms/step - accuracy: 0.9939 - loss: 0.0180 - precision: 0.9943 - val_accuracy: 0.9800 - val_loss: 0.0815 - val_precision: 0.9804\n",
      "Epoch 9/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 4ms/step - accuracy: 0.9954 - loss: 0.0137 - precision: 0.9957 - val_accuracy: 0.9804 - val_loss: 0.0804 - val_precision: 0.9816\n",
      "Epoch 10/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 5ms/step - accuracy: 0.9956 - loss: 0.0143 - precision: 0.9959 - val_accuracy: 0.9742 - val_loss: 0.1156 - val_precision: 0.9750\n",
      "Epoch 11/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 5ms/step - accuracy: 0.9946 - loss: 0.0161 - precision: 0.9948 - val_accuracy: 0.9814 - val_loss: 0.1002 - val_precision: 0.9818\n",
      "Epoch 12/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 0.9973 - loss: 0.0083 - precision: 0.9975 - val_accuracy: 0.9754 - val_loss: 0.1130 - val_precision: 0.9762\n",
      "Epoch 13/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 0.9973 - loss: 0.0093 - precision: 0.9975 - val_accuracy: 0.9814 - val_loss: 0.0833 - val_precision: 0.9826\n",
      "Epoch 14/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 0.9982 - loss: 0.0057 - precision: 0.9983 - val_accuracy: 0.9814 - val_loss: 0.0972 - val_precision: 0.9814\n",
      "Epoch 15/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 0.9982 - loss: 0.0061 - precision: 0.9982 - val_accuracy: 0.9826 - val_loss: 0.0894 - val_precision: 0.9830\n",
      "Epoch 16/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step - accuracy: 0.9986 - loss: 0.0046 - precision: 0.9986 - val_accuracy: 0.9798 - val_loss: 0.1019 - val_precision: 0.9802\n",
      "Epoch 17/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 4ms/step - accuracy: 0.9986 - loss: 0.0043 - precision: 0.9987 - val_accuracy: 0.9840 - val_loss: 0.0861 - val_precision: 0.9848\n",
      "Epoch 18/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 0.9997 - loss: 0.0015 - precision: 0.9997 - val_accuracy: 0.9836 - val_loss: 0.0859 - val_precision: 0.9842\n",
      "Epoch 19/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 1.0000 - loss: 1.8414e-04 - precision: 1.0000 - val_accuracy: 0.9846 - val_loss: 0.0849 - val_precision: 0.9846\n",
      "Epoch 20/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 4ms/step - accuracy: 1.0000 - loss: 9.3367e-05 - precision: 1.0000 - val_accuracy: 0.9840 - val_loss: 0.0853 - val_precision: 0.9842\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.9853 - loss: 0.0802 - precision: 0.9856\n",
      "\n",
      "Final Test Accuracy: 0.9853\n",
      "Final Test Precision: 0.9856\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 2: The 100-Layer Challenge & Vanishing Gradients\n",
    "\n",
    "## Step 2: Deep Architecture Analysis\n",
    "\n",
    "**Objective:** Understanding why modern architectures need specialized activation functions.\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "In very deep networks, gradients diminish as they propagate backward through layers. By the time the gradient signal reaches the initial layers, it approaches zero—effectively stopping the model from learning in early layers.\n",
    "\n",
    "#### 1. The \"Vanishing Gradient\" Trap (Sigmoid)\n",
    "If you train the 100-layer model with Sigmoid, you will likely see the accuracy stay stuck at exactly 10%.\n",
    "\n",
    "Why: In a 100-layer network, the \"signal\" from the error at the end of the model gets smaller and smaller as it travels backward toward the first layer.\n",
    "\n",
    "The Result: By the time the math reaches the early layers, the update value is so tiny (vanishing) that the weights never change. The model stays in a state of \"random guessing,\" and since there are 10 classes in MNIST, a random guess is 10% accurate.\n",
    "\n",
    "#### 2. The \"Self-Normalizing\" Miracle (SELU)\n",
    "If you switch to SELU (Scaled Exponential Linear Unit), the model should actually start learning.\n",
    "\n",
    "Why: SELU, when paired with kernel_initializer=\"lecun_normal\", has a special mathematical property: it keeps the mean and variance of the neuron outputs consistent across all 100 layers.\n",
    "\n",
    "The Result: The gradient doesn't explode or vanish, allowing the information to flow through all 100 layers. You will see the accuracy climb from 10% to 90%+.\n",
    "\n",
    "### Activation Function Comparison\n",
    "\n",
    "| Activation | Characteristics | Vanishing Gradient Issue |\n",
    "|------------|-----------------|--------------------------|\n",
    "| **Sigmoid** | Saturates at 0 or 1 | Severe - gradients vanish due to saturation |\n",
    "| **ReLU** | Positive values pass, negatives become 0 | Moderate - can cause \"Dying ReLU\" where neurons get stuck at 0 |\n",
    "| **ELU/SELU** | Allows negative values, keeps mean activation near zero | Minimal - SELU provides \"Self-Normalization\" for deep networks |"
   ],
   "id": "301e913566aada8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-20T13:05:51.592810880Z",
     "start_time": "2026-02-20T12:48:42.840828834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to build a super deep 100-layer model\n",
    "def build_deep_model(activation):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "    # Add 100 hidden layers\n",
    "    for _ in range(100):\n",
    "        model.add(keras.layers.Dense(100, activation=activation, kernel_initializer=\"he_normal\" if activation != \"sigmoid\" else \"glorot_uniform\"))\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "# Practice: Run this for 'sigmoid' then 'selu'\n",
    "# Note: Sigmoid will likely show 10% accuracy (random guessing) because it can't train 100 layers.\n",
    "# model_deep = build_deep_model(\"selu\")\n",
    "model_deep = build_deep_model(\"sigmoid\")\n",
    "model_deep.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# --- TEST 1: SIGMOID ---\n",
    "print(\"Training 100-layer model with SIGMOID...\")\n",
    "model_sigmoid = build_deep_model(\"sigmoid\")\n",
    "model_sigmoid.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                      optimizer=\"nadam\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "# Pass the actual data variables here!\n",
    "model_sigmoid.fit(X_train, y_train, epochs=15, validation_data=(X_valid, y_valid))\n",
    "\n",
    "\n",
    "# --- TEST 2: SELU ---\n",
    "print(\"\\nTraining 100-layer model with SELU...\")\n",
    "model_selu = build_deep_model(\"selu\")\n",
    "model_selu.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                   optimizer=\"nadam\",\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "# Pass the actual data variables here!\n",
    "model_selu.fit(X_train, y_train, epochs=15, validation_data=(X_valid, y_valid))"
   ],
   "id": "816e7df923644758",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 100-layer model with SIGMOID...\n",
      "Epoch 1/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m67s\u001B[0m 18ms/step - accuracy: 0.1028 - loss: 2.3100 - val_accuracy: 0.0988 - val_loss: 2.3124\n",
      "Epoch 2/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 17ms/step - accuracy: 0.1022 - loss: 2.3077 - val_accuracy: 0.0964 - val_loss: 2.3040\n",
      "Epoch 3/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 19ms/step - accuracy: 0.1063 - loss: 2.3057 - val_accuracy: 0.0966 - val_loss: 2.3054\n",
      "Epoch 4/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 18ms/step - accuracy: 0.1093 - loss: 2.3028 - val_accuracy: 0.1060 - val_loss: 2.3026\n",
      "Epoch 5/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 18ms/step - accuracy: 0.1123 - loss: 2.3016 - val_accuracy: 0.1060 - val_loss: 2.3020\n",
      "Epoch 6/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 17ms/step - accuracy: 0.1129 - loss: 2.3014 - val_accuracy: 0.1060 - val_loss: 2.3019\n",
      "Epoch 7/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 17ms/step - accuracy: 0.1129 - loss: 2.3013 - val_accuracy: 0.1060 - val_loss: 2.3027\n",
      "Epoch 8/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 17ms/step - accuracy: 0.1129 - loss: 2.3013 - val_accuracy: 0.1060 - val_loss: 2.3018\n",
      "Epoch 9/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 18ms/step - accuracy: 0.1129 - loss: 2.3013 - val_accuracy: 0.1060 - val_loss: 2.3017\n",
      "Epoch 10/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 18ms/step - accuracy: 0.1129 - loss: 2.3013 - val_accuracy: 0.1060 - val_loss: 2.3018\n",
      "Epoch 11/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 20ms/step - accuracy: 0.1129 - loss: 2.3013 - val_accuracy: 0.1060 - val_loss: 2.3019\n",
      "Epoch 12/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 18ms/step - accuracy: 0.1129 - loss: 2.3013 - val_accuracy: 0.1060 - val_loss: 2.3018\n",
      "Epoch 13/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 19ms/step - accuracy: 0.1129 - loss: 2.3013 - val_accuracy: 0.1060 - val_loss: 2.3017\n",
      "Epoch 14/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 18ms/step - accuracy: 0.1129 - loss: 2.3013 - val_accuracy: 0.1060 - val_loss: 2.3020\n",
      "Epoch 15/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 17ms/step - accuracy: 0.1129 - loss: 2.3013 - val_accuracy: 0.1060 - val_loss: 2.3019\n",
      "\n",
      "Training 100-layer model with SELU...\n",
      "Epoch 1/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m68s\u001B[0m 19ms/step - accuracy: 0.3251 - loss: 2.4825 - val_accuracy: 0.5118 - val_loss: 1.2415\n",
      "Epoch 2/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 19ms/step - accuracy: 0.5151 - loss: 1.2998 - val_accuracy: 0.4570 - val_loss: 1.5376\n",
      "Epoch 3/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 19ms/step - accuracy: 0.2944 - loss: 1.8970 - val_accuracy: 0.0966 - val_loss: 2.2671\n",
      "Epoch 4/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 18ms/step - accuracy: 0.4171 - loss: 1.4742 - val_accuracy: 0.4144 - val_loss: 1.5278\n",
      "Epoch 5/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 19ms/step - accuracy: 0.6378 - loss: 1.0276 - val_accuracy: 0.7642 - val_loss: 0.6844\n",
      "Epoch 6/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 18ms/step - accuracy: 0.4810 - loss: 3.1874 - val_accuracy: 0.2240 - val_loss: 1.8228\n",
      "Epoch 7/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 18ms/step - accuracy: 0.4498 - loss: 1.3591 - val_accuracy: 0.7056 - val_loss: 0.9201\n",
      "Epoch 8/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 20ms/step - accuracy: 0.5212 - loss: 1.2870 - val_accuracy: 0.6090 - val_loss: 1.0225\n",
      "Epoch 9/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 20ms/step - accuracy: 0.4807 - loss: 1.3772 - val_accuracy: 0.6736 - val_loss: 0.9823\n",
      "Epoch 10/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 19ms/step - accuracy: 0.4818 - loss: 1.5543 - val_accuracy: 0.4890 - val_loss: 1.2630\n",
      "Epoch 11/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 19ms/step - accuracy: 0.6060 - loss: 1.1029 - val_accuracy: 0.7018 - val_loss: 0.8788\n",
      "Epoch 12/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 19ms/step - accuracy: 0.6558 - loss: 1.0331 - val_accuracy: 0.7958 - val_loss: 0.6871\n",
      "Epoch 13/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 19ms/step - accuracy: 0.1303 - loss: 6.9035 - val_accuracy: 0.0978 - val_loss: 2.3202\n",
      "Epoch 14/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 19ms/step - accuracy: 0.1006 - loss: 2.3172 - val_accuracy: 0.1018 - val_loss: 2.3282\n",
      "Epoch 15/15\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 17ms/step - accuracy: 0.1042 - loss: 2.3177 - val_accuracy: 0.1060 - val_loss: 2.3253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76e96c27e150>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 3: CIFAR10, Batch Normalization, and Optimizers\n",
    "\n",
    "## Step 3: CIFAR10 and Optimization\n",
    "\n",
    "**Objective:** Train on the more complex CIFAR10 dataset (color images, 3 channels) and address training stability issues.\n",
    "\n",
    "### He Initialization\n",
    "\n",
    "Designed specifically for ELU/ReLU activation functions to prevent signal death and maintain proper gradient flow throughout the network.\n",
    "\n",
    "### Batch Normalization (BN)\n",
    "\n",
    "Standardizes the inputs to each layer, providing:\n",
    "- Ability to use much higher learning rates\n",
    "- Reduced sensitivity to weight initialization\n",
    "- Faster convergence and improved training stability\n",
    "\n",
    "### Optimizer Comparison\n",
    "\n",
    "| Optimizer | Description | Best Use Case |\n",
    "|-----------|-------------|---------------|\n",
    "| **Momentum** | Builds velocity like a ball rolling downhill | Standard SGD with faster convergence |\n",
    "| **Adam/Nadam** | Combines momentum with adaptive learning rates per weight | The \"go-to\" optimizer for most deep learning tasks |"
   ],
   "id": "6deb5dd52e5d1474"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T16:28:20.980396260Z",
     "start_time": "2026-02-14T16:10:50.699230475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load CIFAR10\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "\n",
    "# 2. Build DNN with Batch Normalization\n",
    "model_cifar = keras.Sequential()\n",
    "model_cifar.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "# Add 20 layers with Batch Normalization\n",
    "for _ in range(20):\n",
    "    model_cifar.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\")) # Layer\n",
    "    model_cifar.add(keras.layers.BatchNormalization())                      # Normalization\n",
    "    model_cifar.add(keras.layers.Activation(\"elu\"))                        # Activation\n",
    "\n",
    "model_cifar.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# 3. Train with Nadam\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model_cifar.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Early Stopping to save time\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history_cifar = model_cifar.fit(X_train_full, y_train_full, epochs=50,\n",
    "                                validation_split=0.1, callbacks=[early_stopping_cb])"
   ],
   "id": "da2e38724e1a139b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 17:10:58.480078: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 552960000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 29ms/step - accuracy: 0.3379 - loss: 1.8422 - val_accuracy: 0.3714 - val_loss: 1.7833\n",
      "Epoch 2/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 19ms/step - accuracy: 0.4056 - loss: 1.6603 - val_accuracy: 0.3618 - val_loss: 1.7728\n",
      "Epoch 3/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 37ms/step - accuracy: 0.4335 - loss: 1.5919 - val_accuracy: 0.4034 - val_loss: 1.7095\n",
      "Epoch 4/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m57s\u001B[0m 40ms/step - accuracy: 0.4541 - loss: 1.5405 - val_accuracy: 0.4368 - val_loss: 1.5845\n",
      "Epoch 5/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 37ms/step - accuracy: 0.4690 - loss: 1.5067 - val_accuracy: 0.4168 - val_loss: 1.6746\n",
      "Epoch 6/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m63s\u001B[0m 23ms/step - accuracy: 0.4807 - loss: 1.4671 - val_accuracy: 0.4252 - val_loss: 1.6027\n",
      "Epoch 7/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 21ms/step - accuracy: 0.4925 - loss: 1.4353 - val_accuracy: 0.4770 - val_loss: 1.4744\n",
      "Epoch 8/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 23ms/step - accuracy: 0.5015 - loss: 1.4047 - val_accuracy: 0.4530 - val_loss: 1.5420\n",
      "Epoch 9/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 22ms/step - accuracy: 0.5135 - loss: 1.3766 - val_accuracy: 0.4638 - val_loss: 1.5134\n",
      "Epoch 10/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 21ms/step - accuracy: 0.5172 - loss: 1.3578 - val_accuracy: 0.4442 - val_loss: 1.5947\n",
      "Epoch 11/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 23ms/step - accuracy: 0.5269 - loss: 1.3312 - val_accuracy: 0.4794 - val_loss: 1.4783\n",
      "Epoch 12/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 21ms/step - accuracy: 0.5328 - loss: 1.3090 - val_accuracy: 0.5076 - val_loss: 1.4219\n",
      "Epoch 13/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 20ms/step - accuracy: 0.5459 - loss: 1.2927 - val_accuracy: 0.5032 - val_loss: 1.4313\n",
      "Epoch 14/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 19ms/step - accuracy: 0.5495 - loss: 1.2728 - val_accuracy: 0.4512 - val_loss: 1.6056\n",
      "Epoch 15/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 21ms/step - accuracy: 0.5563 - loss: 1.2553 - val_accuracy: 0.4972 - val_loss: 1.4334\n",
      "Epoch 16/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.5622 - loss: 1.2379 - val_accuracy: 0.4690 - val_loss: 1.5176\n",
      "Epoch 17/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.5675 - loss: 1.2199 - val_accuracy: 0.4810 - val_loss: 1.4996\n",
      "Epoch 18/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.5744 - loss: 1.2043 - val_accuracy: 0.4274 - val_loss: 1.7869\n",
      "Epoch 19/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.5780 - loss: 1.1899 - val_accuracy: 0.5026 - val_loss: 1.4426\n",
      "Epoch 20/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.5838 - loss: 1.1762 - val_accuracy: 0.5112 - val_loss: 1.4014\n",
      "Epoch 21/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 22ms/step - accuracy: 0.5902 - loss: 1.1581 - val_accuracy: 0.4780 - val_loss: 1.5385\n",
      "Epoch 22/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 17ms/step - accuracy: 0.5945 - loss: 1.1437 - val_accuracy: 0.4980 - val_loss: 1.4933\n",
      "Epoch 23/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 16ms/step - accuracy: 0.5961 - loss: 1.1394 - val_accuracy: 0.4876 - val_loss: 1.4838\n",
      "Epoch 24/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.6030 - loss: 1.1240 - val_accuracy: 0.4108 - val_loss: 1.9329\n",
      "Epoch 25/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 22ms/step - accuracy: 0.6098 - loss: 1.1081 - val_accuracy: 0.4834 - val_loss: 1.5184\n",
      "Epoch 26/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.6118 - loss: 1.0956 - val_accuracy: 0.5048 - val_loss: 1.4415\n",
      "Epoch 27/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 21ms/step - accuracy: 0.6180 - loss: 1.0841 - val_accuracy: 0.4910 - val_loss: 1.5153\n",
      "Epoch 28/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.6185 - loss: 1.0784 - val_accuracy: 0.4990 - val_loss: 1.5101\n",
      "Epoch 29/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 24ms/step - accuracy: 0.6254 - loss: 1.0669 - val_accuracy: 0.4934 - val_loss: 1.5433\n",
      "Epoch 30/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.6302 - loss: 1.0546 - val_accuracy: 0.4910 - val_loss: 1.5362\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparison Discussion\n",
    "\n",
    "### Convergence Speed\n",
    "Batch Normalization typically enables the model to reach higher accuracy in fewer epochs, even though each epoch may take slightly longer to compute due to the additional normalization calculations.\n",
    "\n",
    "### Optimizer Differences\n",
    "\n",
    "| Optimizer | Characteristics | Tuning Required |\n",
    "|-----------|-----------------|------------------|\n",
    "| **SGD** | Slow convergence, can get stuck in local minima | High |\n",
    "| **Momentum/NAG** | Faster than SGD, builds velocity, better at escaping local minima | Medium |\n",
    "| **Adam/Nadam** | Most \"forgiving\", combines momentum with adaptive learning rates, fastest convergence | Low |"
   ],
   "id": "2bf7ebc509364295"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
