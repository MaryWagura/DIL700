{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: MNIST with Optimal Learning Rate & TensorBoard\n",
    "\n",
    "## Step 1: MNIST Optimization\n",
    "\n",
    "**Objective:** Reach >98% accuracy on the MNIST dataset.\n",
    "\n",
    "### Learning Rate Finder\n",
    "\n",
    "We implement a callback that exponentially increases the learning rate during training. By plotting Loss vs. Learning Rate, we can identify the optimal learning rate range—typically 10x smaller than the rate where the loss begins to increase.\n",
    "\n",
    "### TensorBoard Integration\n",
    "\n",
    "TensorBoard provides real-time visualization of training metrics including:\n",
    "- Loss curves\n",
    "- Accuracy metrics\n",
    "- Learning rate progression\n",
    "- Model graph structure"
   ],
   "id": "ae50e1734afb1775"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 1. Load MNIST\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0  # Scale to [0, 1]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Create validation set\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "# 2. Build Model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 3. Learning Rate Finder Setup\n",
    "# We grow LR exponentially from 1e-3 to 10\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        keras.backend.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1e-3), metrics=[\"accuracy\"])\n",
    "expon_lr = ExponentialLearningRate(factor=1.005)\n",
    "\n",
    "# Train for one epoch to find the rate\n",
    "model.fit(X_train, y_train, epochs=1, callbacks=[expon_lr])\n",
    "\n",
    "# 4. Plot LR vs Loss\n",
    "plt.plot(expon_lr.rates, expon_lr.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\n",
    "plt.axis([1e-3, 10, 0, 5])\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Final Training with TensorBoard\n",
    "# Use the best LR found (e.g., 3e-1)\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(root_logdir)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=3e-1),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])"
   ],
   "id": "f211f2ca89d32889"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 2: The 100-Layer Challenge & Vanishing Gradients\n",
    "\n",
    "## Step 2: Deep Architecture Analysis\n",
    "\n",
    "**Objective:** Understanding why modern architectures need specialized activation functions.\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "In very deep networks, gradients diminish as they propagate backward through layers. By the time the gradient signal reaches the initial layers, it approaches zero—effectively stopping the model from learning in early layers.\n",
    "\n",
    "### Activation Function Comparison\n",
    "\n",
    "| Activation | Characteristics | Vanishing Gradient Issue |\n",
    "|------------|-----------------|--------------------------|\n",
    "| **Sigmoid** | Saturates at 0 or 1 | Severe - gradients vanish due to saturation |\n",
    "| **ReLU** | Positive values pass, negatives become 0 | Moderate - can cause \"Dying ReLU\" where neurons get stuck at 0 |\n",
    "| **ELU/SELU** | Allows negative values, keeps mean activation near zero | Minimal - SELU provides \"Self-Normalization\" for deep networks |"
   ],
   "id": "301e913566aada8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to build a super deep 100-layer model\n",
    "def build_deep_model(activation):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "    # Add 100 hidden layers\n",
    "    for _ in range(100):\n",
    "        model.add(keras.layers.Dense(100, activation=activation, kernel_initializer=\"he_normal\" if activation != \"sigmoid\" else \"glorot_uniform\"))\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "# Practice: Run this for 'sigmoid' then 'selu'\n",
    "# Note: Sigmoid will likely show 10% accuracy (random guessing) because it can't train 100 layers.\n",
    "model_deep = build_deep_model(\"selu\")\n",
    "model_deep.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "# model_deep.fit(...)"
   ],
   "id": "816e7df923644758"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 3: CIFAR10, Batch Normalization, and Optimizers\n",
    "\n",
    "## Step 3: CIFAR10 and Optimization\n",
    "\n",
    "**Objective:** Train on the more complex CIFAR10 dataset (color images, 3 channels) and address training stability issues.\n",
    "\n",
    "### He Initialization\n",
    "\n",
    "Designed specifically for ELU/ReLU activation functions to prevent signal death and maintain proper gradient flow throughout the network.\n",
    "\n",
    "### Batch Normalization (BN)\n",
    "\n",
    "Standardizes the inputs to each layer, providing:\n",
    "- Ability to use much higher learning rates\n",
    "- Reduced sensitivity to weight initialization\n",
    "- Faster convergence and improved training stability\n",
    "\n",
    "### Optimizer Comparison\n",
    "\n",
    "| Optimizer | Description | Best Use Case |\n",
    "|-----------|-------------|---------------|\n",
    "| **Momentum** | Builds velocity like a ball rolling downhill | Standard SGD with faster convergence |\n",
    "| **Adam/Nadam** | Combines momentum with adaptive learning rates per weight | The \"go-to\" optimizer for most deep learning tasks |"
   ],
   "id": "6deb5dd52e5d1474"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Load CIFAR10\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "\n",
    "# 2. Build DNN with Batch Normalization\n",
    "model_cifar = keras.Sequential()\n",
    "model_cifar.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "# Add 20 layers with Batch Normalization\n",
    "for _ in range(20):\n",
    "    model_cifar.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\")) # Layer\n",
    "    model_cifar.add(keras.layers.BatchNormalization())                      # Normalization\n",
    "    model_cifar.add(keras.layers.Activation(\"elu\"))                        # Activation\n",
    "\n",
    "model_cifar.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# 3. Train with Nadam\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model_cifar.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Early Stopping to save time\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history_cifar = model_cifar.fit(X_train_full, y_train_full, epochs=50,\n",
    "                                validation_split=0.1, callbacks=[early_stopping_cb])"
   ],
   "id": "da2e38724e1a139b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparison Discussion\n",
    "\n",
    "### Convergence Speed\n",
    "Batch Normalization typically enables the model to reach higher accuracy in fewer epochs, even though each epoch may take slightly longer to compute due to the additional normalization calculations.\n",
    "\n",
    "### Optimizer Differences\n",
    "\n",
    "| Optimizer | Characteristics | Tuning Required |\n",
    "|-----------|-----------------|------------------|\n",
    "| **SGD** | Slow convergence, can get stuck in local minima | High |\n",
    "| **Momentum/NAG** | Faster than SGD, builds velocity, better at escaping local minima | Medium |\n",
    "| **Adam/Nadam** | Most \"forgiving\", combines momentum with adaptive learning rates, fastest convergence | Low |"
   ],
   "id": "2bf7ebc509364295"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
