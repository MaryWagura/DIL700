{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: MNIST with Optimal Learning Rate & TensorBoard\n",
    "\n",
    "## Step 1: MNIST Optimization\n",
    "\n",
    "**Objective:** Reach >98% accuracy on the MNIST dataset.\n",
    "\n",
    "### Learning Rate Finder\n",
    "\n",
    "We implement a callback that exponentially increases the learning rate during training. By plotting Loss vs. Learning Rate, we can identify the optimal learning rate range—typically 10x smaller than the rate where the loss begins to increase.\n",
    "\n",
    "### TensorBoard Integration\n",
    "\n",
    "TensorBoard provides real-time visualization of training metrics including:\n",
    "- Loss curves\n",
    "- Accuracy metrics\n",
    "- Learning rate progression\n",
    "- Model graph structure"
   ],
   "id": "ae50e1734afb1775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Data Loading and Preprocessing",
   "id": "2a2e2fbd742bc558"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T16:09:55.568319499Z",
     "start_time": "2026-02-14T16:09:54.636247724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 1. Load MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# 2. Scale pixel values to [0, 1] range\n",
    "# Neural networks perform better with small, normalized input values\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# 3. Create a validation set (last 5,000 images)\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "print(f\"Data Loaded: {len(X_train)} training samples, {len(X_valid)} validation samples\")"
   ],
   "id": "d1b55da27f635ea4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded: 55000 training samples, 5000 validation samples\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Define the Learning Rate Finder Callback",
   "id": "3e7269c32ca918cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T15:59:49.857117400Z",
     "start_time": "2026-02-14T15:59:49.692847189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom callback to grow learning rate exponentially during training\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        super().__init__() # Ensure parent class is initialized\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # 1. Access the learning rate variable directly from the optimizer\n",
    "        lr_variable = self.model.optimizer.learning_rate\n",
    "\n",
    "        # 2. Convert to float safely (Works across Keras 2 and 3)\n",
    "        current_lr = float(lr_variable)\n",
    "\n",
    "        self.rates.append(current_lr)\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "\n",
    "        # 3. Use .assign() to update the learning rate for the next batch\n",
    "        # This is the modern, safe way to change variables in Keras/TensorFlow\n",
    "        lr_variable.assign(current_lr * self.factor)"
   ],
   "id": "f2caebc82464a331",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Find the Optimal Learning Rate",
   "id": "5ab51786282dce2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T16:00:25.586042855Z",
     "start_time": "2026-02-14T16:00:02.646479043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Build a basic MLP model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 2. Compile with a low starting LR\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Run the LR finder for 1 epoch\n",
    "expon_lr = ExponentialLearningRate(factor=1.005)\n",
    "model.fit(X_train, y_train, epochs=1, callbacks=[expon_lr])\n",
    "\n",
    "# 4. Plot the results\n",
    "plt.plot(expon_lr.rates, expon_lr.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.axis([1e-3, 10, 0, 5])\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Finding the Optimal Learning Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "51899719544425ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 7ms/step - accuracy: 0.6090 - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Final Training with TensorBoard and Precision MetricNote: Look at your plot above. Usually, a good choice is about 1/10th of the rate where the loss started to explode. For MNIST, this is often around $3 \\times 10^{-1}$.",
   "id": "b8813446d08bb956"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "In deep learning laboratory settings, labels are typically transformed using **One-Hot Encoding**. This converts a single numeric label (e.g., class `5`) into a binary vector:\n"
   ],
   "id": "9d35614c38af85ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T16:09:54.602961228Z",
     "start_time": "2026-02-14T16:04:45.233902114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. One-Hot Encode the labels to fix the Precision metric shape error\n",
    "# This turns integers (0-9) into 10-element vectors\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_valid_cat = keras.utils.to_categorical(y_valid, num_classes=10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# 2. Define TensorBoard log directory\n",
    "root_logdir = os.path.join(os.curdir, \"my_mnist_logs\")\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(root_logdir)\n",
    "\n",
    "# 3. Re-build/Reset the model\n",
    "model = keras.models.clone_model(model)\n",
    "\n",
    "# 4. Compile with Categorical Crossentropy and standard Precision\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", # Changed from sparse because of one-hot labels\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=3e-1),\n",
    "    metrics=[\"accuracy\", keras.metrics.Precision(name=\"precision\")]\n",
    ")\n",
    "\n",
    "# 5. Final Training\n",
    "# We use the categorical labels here\n",
    "history = model.fit(X_train, y_train_cat, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid_cat),\n",
    "                    callbacks=[tensorboard_cb])\n",
    "\n",
    "# 6. Evaluate on Test Set\n",
    "test_results = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"\\nFinal Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"Final Test Precision: {test_results[2]:.4f}\")"
   ],
   "id": "f732adb825f26f5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 8ms/step - accuracy: 0.9281 - loss: 0.2305 - precision: 0.9492 - val_accuracy: 0.9718 - val_loss: 0.0959 - val_precision: 0.9758\n",
      "Epoch 2/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 9ms/step - accuracy: 0.9707 - loss: 0.0947 - precision: 0.9750 - val_accuracy: 0.9712 - val_loss: 0.0995 - val_precision: 0.9746\n",
      "Epoch 3/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 8ms/step - accuracy: 0.9792 - loss: 0.0644 - precision: 0.9821 - val_accuracy: 0.9748 - val_loss: 0.0890 - val_precision: 0.9775\n",
      "Epoch 4/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 8ms/step - accuracy: 0.9843 - loss: 0.0478 - precision: 0.9859 - val_accuracy: 0.9684 - val_loss: 0.1073 - val_precision: 0.9711\n",
      "Epoch 5/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 7ms/step - accuracy: 0.9875 - loss: 0.0383 - precision: 0.9886 - val_accuracy: 0.9768 - val_loss: 0.0873 - val_precision: 0.9785\n",
      "Epoch 6/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 8ms/step - accuracy: 0.9906 - loss: 0.0288 - precision: 0.9913 - val_accuracy: 0.9750 - val_loss: 0.0989 - val_precision: 0.9759\n",
      "Epoch 7/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 9ms/step - accuracy: 0.9923 - loss: 0.0230 - precision: 0.9929 - val_accuracy: 0.9812 - val_loss: 0.0739 - val_precision: 0.9826\n",
      "Epoch 8/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 8ms/step - accuracy: 0.9937 - loss: 0.0188 - precision: 0.9942 - val_accuracy: 0.9802 - val_loss: 0.0862 - val_precision: 0.9818\n",
      "Epoch 9/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 8ms/step - accuracy: 0.9951 - loss: 0.0155 - precision: 0.9954 - val_accuracy: 0.9836 - val_loss: 0.0743 - val_precision: 0.9844\n",
      "Epoch 10/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 8ms/step - accuracy: 0.9953 - loss: 0.0140 - precision: 0.9955 - val_accuracy: 0.9808 - val_loss: 0.0948 - val_precision: 0.9810\n",
      "Epoch 11/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 9ms/step - accuracy: 0.9964 - loss: 0.0111 - precision: 0.9967 - val_accuracy: 0.9826 - val_loss: 0.0843 - val_precision: 0.9838\n",
      "Epoch 12/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 10ms/step - accuracy: 0.9974 - loss: 0.0078 - precision: 0.9975 - val_accuracy: 0.9820 - val_loss: 0.0931 - val_precision: 0.9822\n",
      "Epoch 13/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m19s\u001B[0m 11ms/step - accuracy: 0.9963 - loss: 0.0113 - precision: 0.9964 - val_accuracy: 0.9844 - val_loss: 0.0889 - val_precision: 0.9848\n",
      "Epoch 14/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 10ms/step - accuracy: 0.9963 - loss: 0.0112 - precision: 0.9964 - val_accuracy: 0.9844 - val_loss: 0.0866 - val_precision: 0.9852\n",
      "Epoch 15/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 10ms/step - accuracy: 0.9966 - loss: 0.0104 - precision: 0.9967 - val_accuracy: 0.9830 - val_loss: 0.1018 - val_precision: 0.9836\n",
      "Epoch 16/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 10ms/step - accuracy: 0.9962 - loss: 0.0119 - precision: 0.9964 - val_accuracy: 0.9830 - val_loss: 0.0932 - val_precision: 0.9834\n",
      "Epoch 17/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 10ms/step - accuracy: 0.9962 - loss: 0.0115 - precision: 0.9964 - val_accuracy: 0.9820 - val_loss: 0.0991 - val_precision: 0.9830\n",
      "Epoch 18/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 9ms/step - accuracy: 0.9967 - loss: 0.0086 - precision: 0.9969 - val_accuracy: 0.9834 - val_loss: 0.1065 - val_precision: 0.9838\n",
      "Epoch 19/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 9ms/step - accuracy: 0.9975 - loss: 0.0081 - precision: 0.9976 - val_accuracy: 0.9846 - val_loss: 0.1067 - val_precision: 0.9852\n",
      "Epoch 20/20\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 9ms/step - accuracy: 0.9975 - loss: 0.0079 - precision: 0.9976 - val_accuracy: 0.9848 - val_loss: 0.0923 - val_precision: 0.9850\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 5ms/step - accuracy: 0.9808 - loss: 0.1007 - precision: 0.9816\n",
      "\n",
      "Final Test Accuracy: 0.9808\n",
      "Final Test Precision: 0.9816\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 2: The 100-Layer Challenge & Vanishing Gradients\n",
    "\n",
    "## Step 2: Deep Architecture Analysis\n",
    "\n",
    "**Objective:** Understanding why modern architectures need specialized activation functions.\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "In very deep networks, gradients diminish as they propagate backward through layers. By the time the gradient signal reaches the initial layers, it approaches zero—effectively stopping the model from learning in early layers.\n",
    "\n",
    "### Activation Function Comparison\n",
    "\n",
    "| Activation | Characteristics | Vanishing Gradient Issue |\n",
    "|------------|-----------------|--------------------------|\n",
    "| **Sigmoid** | Saturates at 0 or 1 | Severe - gradients vanish due to saturation |\n",
    "| **ReLU** | Positive values pass, negatives become 0 | Moderate - can cause \"Dying ReLU\" where neurons get stuck at 0 |\n",
    "| **ELU/SELU** | Allows negative values, keeps mean activation near zero | Minimal - SELU provides \"Self-Normalization\" for deep networks |"
   ],
   "id": "301e913566aada8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T16:10:28.349773915Z",
     "start_time": "2026-02-14T16:10:23.539064319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to build a super deep 100-layer model\n",
    "def build_deep_model(activation):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "    # Add 100 hidden layers\n",
    "    for _ in range(100):\n",
    "        model.add(keras.layers.Dense(100, activation=activation, kernel_initializer=\"he_normal\" if activation != \"sigmoid\" else \"glorot_uniform\"))\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "# Practice: Run this for 'sigmoid' then 'selu'\n",
    "# Note: Sigmoid will likely show 10% accuracy (random guessing) because it can't train 100 layers.\n",
    "model_deep = build_deep_model(\"selu\")\n",
    "model_deep.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "# model_deep.fit(...)"
   ],
   "id": "816e7df923644758",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 3: CIFAR10, Batch Normalization, and Optimizers\n",
    "\n",
    "## Step 3: CIFAR10 and Optimization\n",
    "\n",
    "**Objective:** Train on the more complex CIFAR10 dataset (color images, 3 channels) and address training stability issues.\n",
    "\n",
    "### He Initialization\n",
    "\n",
    "Designed specifically for ELU/ReLU activation functions to prevent signal death and maintain proper gradient flow throughout the network.\n",
    "\n",
    "### Batch Normalization (BN)\n",
    "\n",
    "Standardizes the inputs to each layer, providing:\n",
    "- Ability to use much higher learning rates\n",
    "- Reduced sensitivity to weight initialization\n",
    "- Faster convergence and improved training stability\n",
    "\n",
    "### Optimizer Comparison\n",
    "\n",
    "| Optimizer | Description | Best Use Case |\n",
    "|-----------|-------------|---------------|\n",
    "| **Momentum** | Builds velocity like a ball rolling downhill | Standard SGD with faster convergence |\n",
    "| **Adam/Nadam** | Combines momentum with adaptive learning rates per weight | The \"go-to\" optimizer for most deep learning tasks |"
   ],
   "id": "6deb5dd52e5d1474"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T16:28:20.980396260Z",
     "start_time": "2026-02-14T16:10:50.699230475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load CIFAR10\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "\n",
    "# 2. Build DNN with Batch Normalization\n",
    "model_cifar = keras.Sequential()\n",
    "model_cifar.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "# Add 20 layers with Batch Normalization\n",
    "for _ in range(20):\n",
    "    model_cifar.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\")) # Layer\n",
    "    model_cifar.add(keras.layers.BatchNormalization())                      # Normalization\n",
    "    model_cifar.add(keras.layers.Activation(\"elu\"))                        # Activation\n",
    "\n",
    "model_cifar.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# 3. Train with Nadam\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model_cifar.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Early Stopping to save time\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history_cifar = model_cifar.fit(X_train_full, y_train_full, epochs=50,\n",
    "                                validation_split=0.1, callbacks=[early_stopping_cb])"
   ],
   "id": "da2e38724e1a139b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 17:10:58.480078: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 552960000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 29ms/step - accuracy: 0.3379 - loss: 1.8422 - val_accuracy: 0.3714 - val_loss: 1.7833\n",
      "Epoch 2/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 19ms/step - accuracy: 0.4056 - loss: 1.6603 - val_accuracy: 0.3618 - val_loss: 1.7728\n",
      "Epoch 3/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 37ms/step - accuracy: 0.4335 - loss: 1.5919 - val_accuracy: 0.4034 - val_loss: 1.7095\n",
      "Epoch 4/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m57s\u001B[0m 40ms/step - accuracy: 0.4541 - loss: 1.5405 - val_accuracy: 0.4368 - val_loss: 1.5845\n",
      "Epoch 5/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 37ms/step - accuracy: 0.4690 - loss: 1.5067 - val_accuracy: 0.4168 - val_loss: 1.6746\n",
      "Epoch 6/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m63s\u001B[0m 23ms/step - accuracy: 0.4807 - loss: 1.4671 - val_accuracy: 0.4252 - val_loss: 1.6027\n",
      "Epoch 7/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 21ms/step - accuracy: 0.4925 - loss: 1.4353 - val_accuracy: 0.4770 - val_loss: 1.4744\n",
      "Epoch 8/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 23ms/step - accuracy: 0.5015 - loss: 1.4047 - val_accuracy: 0.4530 - val_loss: 1.5420\n",
      "Epoch 9/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 22ms/step - accuracy: 0.5135 - loss: 1.3766 - val_accuracy: 0.4638 - val_loss: 1.5134\n",
      "Epoch 10/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 21ms/step - accuracy: 0.5172 - loss: 1.3578 - val_accuracy: 0.4442 - val_loss: 1.5947\n",
      "Epoch 11/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 23ms/step - accuracy: 0.5269 - loss: 1.3312 - val_accuracy: 0.4794 - val_loss: 1.4783\n",
      "Epoch 12/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 21ms/step - accuracy: 0.5328 - loss: 1.3090 - val_accuracy: 0.5076 - val_loss: 1.4219\n",
      "Epoch 13/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 20ms/step - accuracy: 0.5459 - loss: 1.2927 - val_accuracy: 0.5032 - val_loss: 1.4313\n",
      "Epoch 14/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 19ms/step - accuracy: 0.5495 - loss: 1.2728 - val_accuracy: 0.4512 - val_loss: 1.6056\n",
      "Epoch 15/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 21ms/step - accuracy: 0.5563 - loss: 1.2553 - val_accuracy: 0.4972 - val_loss: 1.4334\n",
      "Epoch 16/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.5622 - loss: 1.2379 - val_accuracy: 0.4690 - val_loss: 1.5176\n",
      "Epoch 17/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.5675 - loss: 1.2199 - val_accuracy: 0.4810 - val_loss: 1.4996\n",
      "Epoch 18/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.5744 - loss: 1.2043 - val_accuracy: 0.4274 - val_loss: 1.7869\n",
      "Epoch 19/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.5780 - loss: 1.1899 - val_accuracy: 0.5026 - val_loss: 1.4426\n",
      "Epoch 20/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.5838 - loss: 1.1762 - val_accuracy: 0.5112 - val_loss: 1.4014\n",
      "Epoch 21/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 22ms/step - accuracy: 0.5902 - loss: 1.1581 - val_accuracy: 0.4780 - val_loss: 1.5385\n",
      "Epoch 22/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 17ms/step - accuracy: 0.5945 - loss: 1.1437 - val_accuracy: 0.4980 - val_loss: 1.4933\n",
      "Epoch 23/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 16ms/step - accuracy: 0.5961 - loss: 1.1394 - val_accuracy: 0.4876 - val_loss: 1.4838\n",
      "Epoch 24/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.6030 - loss: 1.1240 - val_accuracy: 0.4108 - val_loss: 1.9329\n",
      "Epoch 25/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 22ms/step - accuracy: 0.6098 - loss: 1.1081 - val_accuracy: 0.4834 - val_loss: 1.5184\n",
      "Epoch 26/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 23ms/step - accuracy: 0.6118 - loss: 1.0956 - val_accuracy: 0.5048 - val_loss: 1.4415\n",
      "Epoch 27/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 21ms/step - accuracy: 0.6180 - loss: 1.0841 - val_accuracy: 0.4910 - val_loss: 1.5153\n",
      "Epoch 28/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.6185 - loss: 1.0784 - val_accuracy: 0.4990 - val_loss: 1.5101\n",
      "Epoch 29/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 24ms/step - accuracy: 0.6254 - loss: 1.0669 - val_accuracy: 0.4934 - val_loss: 1.5433\n",
      "Epoch 30/50\n",
      "\u001B[1m1407/1407\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 22ms/step - accuracy: 0.6302 - loss: 1.0546 - val_accuracy: 0.4910 - val_loss: 1.5362\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparison Discussion\n",
    "\n",
    "### Convergence Speed\n",
    "Batch Normalization typically enables the model to reach higher accuracy in fewer epochs, even though each epoch may take slightly longer to compute due to the additional normalization calculations.\n",
    "\n",
    "### Optimizer Differences\n",
    "\n",
    "| Optimizer | Characteristics | Tuning Required |\n",
    "|-----------|-----------------|------------------|\n",
    "| **SGD** | Slow convergence, can get stuck in local minima | High |\n",
    "| **Momentum/NAG** | Faster than SGD, builds velocity, better at escaping local minima | Medium |\n",
    "| **Adam/Nadam** | Most \"forgiving\", combines momentum with adaptive learning rates, fastest convergence | Low |"
   ],
   "id": "2bf7ebc509364295"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
