{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 1 - Regularization Techniques\n",
    "\n",
    "## Objective\n",
    "\n",
    "This lab applies four major regularization techniques to the deep models built in Lab 3 to combat overfitting:\n",
    "\n",
    "- **ℓ₁ and ℓ₂ Regularization:** Adding a penalty to the loss function based on the size of the weights.\n",
    "- **Dropout:** Randomly \"killing\" neurons during training to ensure the network doesn't over-rely on specific paths.\n",
    "- **Max-Norm Regularization:** Constraining the weights of each neuron so they don't grow too large.\n",
    "\n",
    "## Conceptual Background\n",
    "\n",
    "| Technique | Description | Effect |\n",
    "|-----------|-------------|--------|\n",
    "| **ℓ₁ Regularization** | Adds penalty proportional to absolute weight values | Produces sparse models (weights become exactly zero), effectively acting as feature selection |\n",
    "| **ℓ₂ Regularization** | Adds penalty proportional to squared weight values | Keeps weights small but rarely zero, helps handle multicollinearity |\n",
    "| **Dropout** | Randomly deactivates neurons during training | Forces the network to learn redundant representations, making it more robust |\n",
    "| **Max-Norm** | Rescales weight vector w such that \\|\\|w\\|\\|₂ ≤ c | Prevents weights from growing too large, where c is the max-norm hyperparameter |"
   ],
   "id": "163f4f0cbea259ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: $\\ell_1$ and $\\ell_2$\n",
    "## RegularizationWe will apply these to a Dense network similar to your MNIST/CIFAR-10 tasks."
   ],
   "id": "278c1327f80b7b74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T07:47:16.208220789Z",
     "start_time": "2026-02-19T07:47:11.699059340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# 1. Define l2 regularization factor\n",
    "# This adds a penalty to the loss: Loss + 0.01 * sum(weights^2)\n",
    "regularizer = keras.regularizers.l2(0.01)\n",
    "\n",
    "model_l2 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[32, 32, 3]), # Using CIFAR-10 shape as example\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=regularizer),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=regularizer),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# If you want to use l1, simply use: keras.regularizers.l1(0.01)\n",
    "# For both (Elastic Net style): keras.regularizers.l1_l2(0.01, 0.01)\n",
    "\n",
    "model_l2.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ],
   "id": "3209cb1c10e1ece6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 08:47:12.089219: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/clauds/anaconda3/envs/tf_env/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Dropout\n",
    "### Dropout is typically placed after each hidden layer."
   ],
   "id": "d5a440940e632455"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T07:48:13.856592984Z",
     "start_time": "2026-02-19T07:48:13.750719982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dropout = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    # 20% of the neurons will be dropped during each training step\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_dropout.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ],
   "id": "1dd08e2c7c26e03e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Max-Norm Regularization\n",
    "### Max-norm doesn't add a penalty to the loss function; instead, it constrains the weights directly after each update."
   ],
   "id": "962220f4e842782"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T07:49:03.798930401Z",
     "start_time": "2026-02-19T07:49:03.597127573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the constraint (e.g., max value of 1.0)\n",
    "max_norm_reg = keras.constraints.max_norm(1.0)\n",
    "\n",
    "model_max_norm = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_constraint=max_norm_reg),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_constraint=max_norm_reg),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_max_norm.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ],
   "id": "b86bb084d5cf7555",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 2 - Data Augmentation\n",
    "\n",
    "## Objective\n",
    "\n",
    "This lab demonstrates how to use TensorFlow Flowers to practice data augmentation. We will explore three ways to implement this:\n",
    "\n",
    "- **Integrated Layers:** Adding augmentation directly into the Sequential model.\n",
    "- **Dataset Mapping:** Applying transformations to the `tf.data` pipeline for better performance.\n",
    "- **Custom Augmentation:** Using `tf.image` for fine-grained control over specific transformations.\n",
    "\n",
    "## Why use Data Augmentation?\n",
    "\n",
    "Data Augmentation is a powerful technique used to artificially expand the size of your training set by creating \"new\" images from existing ones using transformations like rotation, flipping, and zooming. This prevents the model from memorizing specific orientations and helps it generalize better to real-world photos.\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Reduces Overfitting** | By showing the model a slightly different version of the image every time, it can't rely on the exact position of pixels |\n",
    "| **Invariance** | The model learns that a flower is still the same flower even if it is upside down, flipped, or slightly rotated |"
   ],
   "id": "7e0a1a2a19dbb499"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Load and Pre-process the Flowers Dataset",
   "id": "5fab452f1844dbc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 1. Load the tf_flowers dataset\n",
    "# as_supervised=True gives us (image, label) pairs instead of a dictionary\n",
    "(train_ds, val_ds), ds_info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    #Take everything from the very beginning up to the 80% mark. This becomes your Training Set. Take everything starting from the 80% mark until the very end. This becomes your Validation Set.\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# 2. Define image size and batch size\n",
    "IMG_SIZE = 180\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 3. Resize and Rescale function\n",
    "# Images come in different sizes; we need them uniform for the neural network\n",
    "resize_and_rescale = keras.Sequential([\n",
    "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
    "  layers.Rescaling(1./255) # Scales pixels from [0, 255] to [0, 1]\n",
    "])"
   ],
   "id": "90865396473657c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
